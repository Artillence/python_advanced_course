{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c93aa83",
   "metadata": {},
   "source": [
    "# Day 3 - Parallel and High-Performance Python: Threads, AsyncIO, NumPy, Numba, GPUs\n",
    "\n",
    "Welcome to Day 3 of the advanced Python course. Today we focus on **performance** and **parallelism**.\n",
    "\n",
    "We will connect Python language features with how modern CPUs and GPUs execute code. The examples are oriented around physics and mechanical **size/length measurement** data (surface roughness, thickness, diameter measurements, repeated measurements, etc.).\n",
    "\n",
    "## What we will cover today\n",
    "\n",
    "- CPU vs I/O bound work and a high level mental model of performance\n",
    "- The Global Interpreter Lock (GIL) and why it matters\n",
    "- Threads, processes, and when to use which\n",
    "- AsyncIO for I/O-bound tasks (simulated sensor queries)\n",
    "- NumPy vectorized computations for numerical work\n",
    "- Numba JIT compilation for accelerating pure Python loops\n",
    "- A short overview of GPU tools: **cuDF**, **CuPy** and where they fit\n",
    "- A look at Python 3.13 and the future: experimental JIT and optional GIL\n",
    "- A complex end-to-end example: processing multiple measurement files in parallel\n",
    "\n",
    "## Daily agenda and course flow\n",
    "\n",
    "**09:00 - 10:30 (1h 30m)**  \n",
    "- CPU vs I/O bound tasks, performance model\n",
    "- GIL recap and impact on threading\n",
    "- Threads for I/O-bound workloads\n",
    "\n",
    "**10:30 - 10:45 (15m)**  \n",
    "- Short break\n",
    "\n",
    "**10:45 - 12:00 (1h 15m)**  \n",
    "- Processes for CPU-bound workloads\n",
    "- Intro to AsyncIO and simulated asynchronous measurements\n",
    "\n",
    "**12:00 - 13:00 (1h)**  \n",
    "- Lunch break\n",
    "\n",
    "**13:00 - 14:45 (1h 45m)**  \n",
    "- NumPy recap and vectorized computations for measurement data\n",
    "- Practical NumPy vectorization patterns and exercises (physics themed)\n",
    "\n",
    "**14:45 - 15:00 (15m)**  \n",
    "- Short break\n",
    "\n",
    "**15:00 - 16:30 (1h 30m)**  \n",
    "- Numba JIT compilation for numerical loops\n",
    "- Overview of GPU tools (cuDF, CuPy) and limitations\n",
    "- Python 3.13: experimental JIT and optional GIL, and what it may mean\n",
    "- Complex example: parallel processing of multiple measurement data sets\n",
    "\n",
    "Throughout the day, we will mark good points to pause, ask questions, or take a sip of water. Try to roughly follow the timing so that we finish comfortably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685d83a",
   "metadata": {},
   "source": [
    "## Topic 1 - Performance mental model: CPU vs I/O, GIL recap\n",
    "\n",
    "In real scientific and engineering work, **performance** usually means one of:\n",
    "\n",
    "- How fast we can process a large dataset (throughput)\n",
    "- How fast we get a single answer (latency)\n",
    "\n",
    "Python performance is strongly influenced by:\n",
    "\n",
    "- The speed of the underlying CPU or GPU\n",
    "- Whether our code is **CPU-bound** or **I/O-bound**\n",
    "- How much work we keep in **C-accelerated libraries** like NumPy\n",
    "- The **Global Interpreter Lock (GIL)** in CPython\n",
    "\n",
    "### CPU-bound vs I/O-bound\n",
    "\n",
    "- **CPU-bound**: most time is spent doing computations on the CPU.\n",
    "  - Example: computing statistics on millions of surface height samples.\n",
    "- **I/O-bound**: most time is spent waiting for input/output.\n",
    "  - Example: reading files from disk or waiting for a measurement device over the network.\n",
    "\n",
    "### GIL recap (very short)\n",
    "\n",
    "The CPython interpreter uses a **Global Interpreter Lock (GIL)**. Only one thread at a time can execute Python bytecode. This means:\n",
    "\n",
    "- Multiple threads do **not** speed up CPU-bound pure Python code.\n",
    "- Threads can still help with I/O-bound tasks, because while one thread waits for I/O, another can run.\n",
    "- Libraries that release the GIL internally (NumPy, some C extensions) can still run in parallel in C.\n",
    "\n",
    "For a deeper dive, see:\n",
    "\n",
    "- CPython GIL overview: https://wiki.python.org/moin/GlobalInterpreterLock\n",
    "- CPython implementation notes: https://docs.python.org/3/c-api/init.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ef409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU-bound function took ~0.114 s\n",
      "I/O-bound function took ~0.201 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def cpu_bound(n: int) -> int:\n",
    "    \"\"\"Fake CPU work: sum of squares up to n.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "def io_bound(delay: float) -> None:\n",
    "    \"\"\"Fake I/O: sleep to simulate waiting for a device or disk.\"\"\"\n",
    "    time.sleep(delay)\n",
    "\n",
    "start = time.perf_counter()\n",
    "cpu_bound(2_000_000)\n",
    "cpu_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "io_bound(0.2)\n",
    "io_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"CPU-bound function took ~{cpu_time:.3f} s\")\n",
    "print(f\"I/O-bound function took ~{io_time:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09689181",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Classify your own task\n",
    "\n",
    "Think about a real task from your work, preferably related to measurement data.\n",
    "\n",
    "1. Describe a task in a **comment** (for example: loading 100 CSV files, averaging repeated measurements, plotting histograms).\n",
    "2. Decide whether it is primarily **CPU-bound** or **I/O-bound** and write that down as a comment.\n",
    "3. Explain in one more comment line why you think so.\n",
    "\n",
    "Use only comments (`#`) in the cell below. No need to write working code yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Describe one real task and classify it as CPU-bound or I/O-bound.\n",
    "# Example (replace with your own):\n",
    "# Task: Load 200 text files with surface height measurements and compute RMS roughness.\n",
    "# Classification: ?\n",
    "# Why: ?\n",
    "\n",
    "# Your description here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example solution (your answer can and should be different)\n",
    "# Task: Load 200 text files with surface height measurements and compute RMS roughness.\n",
    "# Classification: Mixed, but mostly I/O-bound when reading files, then CPU-bound for the calculations.\n",
    "# Why: Reading data from disk is relatively slow (I/O-bound). Once data is in memory, computing RMS on\n",
    "#      millions of points can be CPU-heavy.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4152a",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Rough timing experiment\n",
    "\n",
    "In this exercise you will run a tiny timing experiment to get a feel for the numbers.\n",
    "\n",
    "1. Use the provided `cpu_bound` function.\n",
    "2. Call it with different values, for example `50_000`, `200_000`, `500_000`, and measure the time for each.\n",
    "3. For timing, use `time.perf_counter()` like in the example.\n",
    "4. Print the `n` value and the time for each run.\n",
    "\n",
    "Do this in a simple, sequential loop. The goal is to get a feeling for how quickly Python loops slow down as `n` grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94aeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Advanced exercise starter\n",
    "# TODO: run cpu_bound with several n values and measure the time.\n",
    "\n",
    "def cpu_bound(n: int) -> int:\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "ns = [50_000, 200_000, 500_000]\n",
    "\n",
    "# for n in ns:\n",
    "#     start = ...  # time.perf_counter()\n",
    "#     result = cpu_bound(n)\n",
    "#     elapsed = ...  # time.perf_counter() - start\n",
    "#     print(f\"n={n}, elapsed={elapsed:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de941549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=50000, elapsed=0.0027 s\n",
      "n=200000, elapsed=0.0131 s\n",
      "n=500000, elapsed=0.0326 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import time\n",
    "\n",
    "def cpu_bound(n: int) -> int:\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "ns = [50_000, 200_000, 500_000]\n",
    "for n in ns:\n",
    "    start = time.perf_counter()\n",
    "    _ = cpu_bound(n)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"n={n}, elapsed={elapsed:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473ea2f",
   "metadata": {},
   "source": [
    "## Topic 2 - Threads for I/O-bound tasks\n",
    "\n",
    "Because of the GIL, Python threads are usually **not** a good solution for CPU-bound acceleration. But for I/O-bound tasks (waiting for files, network, measurement devices), threads can help hide latency.\n",
    "\n",
    "Typical pattern in lab / measurement setups:\n",
    "\n",
    "- You need to query multiple instruments or devices.\n",
    "- Each device responds in, say, 100 ms.\n",
    "- With sequential code, talking to 5 devices takes roughly 5 x 100 ms.\n",
    "- With threads, you can send requests in parallel and total time can be close to 100 ms.\n",
    "\n",
    "Python module: [`threading`](https://docs.python.org/3/library/threading.html)\n",
    "\n",
    "Note that this does not break the GIL for CPU work, but it is very useful to manage multiple slow I/O operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d575b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor 4: value=1.0141, delay=0.20s\n",
      "Sensor 2: value=0.9092, delay=0.30s\n",
      "Sensor 1: value=0.9078, delay=0.40s\n",
      "Sensor 3: value=1.0567, delay=0.60s\n",
      "Total elapsed (threaded): 0.606 s\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "def measure_sensor(sensor_id: int, delay: float) -> None:\n",
    "    \"\"\"Simulate a measurement: wait 'delay' seconds, then print a value.\"\"\"\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)  # normalized size measurement\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "threads = []\n",
    "start = time.perf_counter()\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    t = threading.Thread(target=measure_sensor, args=(i, d))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Total elapsed (threaded): {elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f009a",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Compare sequential vs threaded measurements\n",
    "\n",
    "Use the `measure_sensor` function idea to compare sequential and threaded execution.\n",
    "\n",
    "1. Re-implement a simple `measure_sensor` that sleeps and prints a message.\n",
    "2. First, call it sequentially in a loop for all delays.\n",
    "3. Then, call it using `threading.Thread` like in the example.\n",
    "4. Measure and print the elapsed time for both cases.\n",
    "\n",
    "You can reuse the list `delays = [0.4, 0.3, 0.6, 0.2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "# TODO: implement sequential and threaded measurement and compare their times.\n",
    "\n",
    "def measure_sensor(sensor_id: int, delay: float) -> None:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "# 1) Sequential run\n",
    "# start = ...\n",
    "# for i, d in enumerate(delays, start=1):\n",
    "#     measure_sensor(i, d)\n",
    "# seq_elapsed = ...\n",
    "# print(f\"Sequential elapsed: {seq_elapsed:.3f} s\")\n",
    "\n",
    "# 2) Threaded run\n",
    "# threads = []\n",
    "# start = ...\n",
    "# for i, d in enumerate(delays, start=1):\n",
    "#     ... create and start threads ...\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "# thr_elapsed = ...\n",
    "# print(f\"Threaded elapsed: {thr_elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de51e38e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor 1: value=1.0886, delay=0.40s\n",
      "Sensor 2: value=0.9940, delay=0.30s\n",
      "Sensor 3: value=0.9065, delay=0.60s\n",
      "Sensor 4: value=1.0301, delay=0.20s\n",
      "Sequential elapsed: 1.505 s\n",
      "Sensor 4: value=1.0565, delay=0.20s\n",
      "Sensor 2: value=0.9084, delay=0.30s\n",
      "Sensor 1: value=0.9360, delay=0.40s\n",
      "Sensor 3: value=1.0763, delay=0.60s\n",
      "Threaded elapsed: 0.608 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "def measure_sensor(sensor_id: int, delay: float) -> None:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "# Sequential\n",
    "start = time.perf_counter()\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    measure_sensor(i, d)\n",
    "seq_elapsed = time.perf_counter() - start\n",
    "print(f\"Sequential elapsed: {seq_elapsed:.3f} s\")\n",
    "\n",
    "# Threaded\n",
    "threads = []\n",
    "start = time.perf_counter()\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    t = threading.Thread(target=measure_sensor, args=(i, d))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "thr_elapsed = time.perf_counter() - start\n",
    "print(f\"Threaded elapsed: {thr_elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174bcf1",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Collect results into a shared list\n",
    "\n",
    "Right now, `measure_sensor` just prints values. In real life, we want to **store** results.\n",
    "\n",
    "1. Modify `measure_sensor` so that it appends `(sensor_id, value)` to a shared list.\n",
    "2. Use a `threading.Lock` to protect the shared list.\n",
    "3. After all threads finish, print the collected list of results.\n",
    "\n",
    "This pattern mimics collecting measurement results from multiple devices into a shared in-memory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb41945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "results = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def measure_and_store(sensor_id: int, delay: float) -> None:\n",
    "    \"\"\"TODO: simulate measurement and safely append to results.\"\"\"\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    # with lock:\n",
    "    #     results.append(...)\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "threads = []\n",
    "# TODO: start threads using measure_and_store and join them, then print results.\n",
    "# for i, d in enumerate(delays, start=1):\n",
    "#     t = threading.Thread(target=measure_and_store, args=(i, d))\n",
    "#     threads.append(t)\n",
    "#     t.start()\n",
    "\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8e31b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected results: [(4, 1.0242552744792301), (2, 0.9700888704122314), (1, 1.0925002232553185), (3, 0.9773592353806989)]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "results = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def measure_and_store(sensor_id: int, delay: float) -> None:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    with lock:\n",
    "        results.append((sensor_id, value))\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "threads = []\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    t = threading.Thread(target=measure_and_store, args=(i, d))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"Collected results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b13168",
   "metadata": {},
   "source": [
    "---\n",
    "# Short break (10:30 - 10:45)\n",
    "\n",
    "Stand up, stretch a bit, drink some water. We continue with processes and AsyncIO afterwards.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9eddc",
   "metadata": {},
   "source": [
    "## Topic 3 - Processes for CPU-bound workloads\n",
    "\n",
    "Because of the GIL, threads do not speed up CPU-bound pure Python loops. For heavy numerical work in pure Python, we can use **processes** instead of threads.\n",
    "\n",
    "Python module: [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html)\n",
    "\n",
    "A **process** has its own Python interpreter and its own GIL, so multiple processes can truly run in parallel on multiple CPU cores.\n",
    "\n",
    "Typical pattern in scientific computing:\n",
    "\n",
    "- Split a large dataset into chunks (e.g. surface height maps for different samples).\n",
    "- Spawn a pool of worker processes.\n",
    "- Each process computes statistics for one chunk.\n",
    "- Collect results in the main process.\n",
    "\n",
    "Downside: processes have more overhead than threads (especially for sending large arrays between processes), but they allow true CPU-core scaling for pure Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7ca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using up to 8 CPU cores\n"
     ]
    }
   ],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "\n",
    "def compute_rms(values):\n",
    "    \"\"\"Compute RMS roughness of a list of heights.\"\"\"\n",
    "    s = 0.0\n",
    "    for x in values:\n",
    "        s += x * x\n",
    "    return math.sqrt(s / len(values))\n",
    "\n",
    "# Create fake measurement data: 4 samples with 100_000 points each\n",
    "import random\n",
    "samples = [[random.uniform(-1e-6, 1e-6) for _ in range(100_000)] for _ in range(4)]\n",
    "\n",
    "print(f\"Using up to {cpu_count()} CPU cores\")\n",
    "with Pool() as pool:\n",
    "    rms_values = pool.map(compute_rms, samples)\n",
    "\n",
    "print(\"RMS roughness per sample:\")\n",
    "for i, rms in enumerate(rms_values, start=1):\n",
    "    print(f\"  Sample {i}: {rms:.3e} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea85b4",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Parallel average diameter per batch\n",
    "\n",
    "Imagine you have several batches of diameter measurements for different parts. Each batch is a list of floats.\n",
    "\n",
    "1. Implement a function `average(values)` that computes the mean of the list.\n",
    "2. Create a list of batches (for example 3-5 lists with random diameters in millimeters).\n",
    "3. Use `multiprocessing.Pool.map` to compute the average diameter for each batch in parallel.\n",
    "4. Print the result for each batch.\n",
    "\n",
    "Keep the batch sizes small enough that the code finishes quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# TODO: compute average diameter per batch in parallel.\n",
    "\n",
    "def average(values):\n",
    "    # return ...\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    return total / len(values)\n",
    "\n",
    "# Create example batches of diameters in mm\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "# with Pool() as pool:\n",
    "#     averages = pool.map(average, batches)\n",
    "# for i, avg in enumerate(averages, start=1):\n",
    "#     print(f\"Batch {i}: average diameter = {avg:.3f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f590d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def average(values):\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    return total / len(values)\n",
    "\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "with Pool() as pool:\n",
    "    averages = pool.map(average, batches)\n",
    "\n",
    "for i, avg in enumerate(averages, start=1):\n",
    "    print(f\"Batch {i}: average diameter = {avg:.3f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cab95",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Parallel min, max, mean\n",
    "\n",
    "Extend the previous idea to compute **three** statistics per batch: `(min, max, mean)`.\n",
    "\n",
    "1. Write a function `stats(values)` that returns a tuple `(min_value, max_value, mean_value)`.\n",
    "2. Reuse or create batches of diameter or thickness measurements.\n",
    "3. Use a process pool to compute stats for each batch in parallel.\n",
    "4. Print the three statistics for each batch in a readable way.\n",
    "\n",
    "This is close to what you would actually do with per-sample measurement datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aee1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def stats(values):\n",
    "    \"\"\"TODO: return (min, max, mean) for the list.\"\"\"\n",
    "    # m = min(values)\n",
    "    # M = max(values)\n",
    "    # mean = ...\n",
    "    # return m, M, mean\n",
    "    m = min(values)\n",
    "    M = max(values)\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    mean = total / len(values)\n",
    "    return m, M, mean\n",
    "\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "# TODO: use Pool to compute stats in parallel and print them.\n",
    "# with Pool() as pool:\n",
    "#     results = pool.map(stats, batches)\n",
    "# for i, (m, M, mean) in enumerate(results, start=1):\n",
    "#     print(f\"Batch {i}: min={m:.3f}, max={M:.3f}, mean={mean:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a11fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def stats(values):\n",
    "    m = min(values)\n",
    "    M = max(values)\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    mean = total / len(values)\n",
    "    return m, M, mean\n",
    "\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "with Pool() as pool:\n",
    "    results = pool.map(stats, batches)\n",
    "\n",
    "for i, (m, M, mean) in enumerate(results, start=1):\n",
    "    print(f\"Batch {i}: min={m:.3f}, max={M:.3f}, mean={mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dea63b",
   "metadata": {},
   "source": [
    "## Topic 4 - AsyncIO for concurrent I/O\n",
    "\n",
    "Python's [`asyncio`](https://docs.python.org/3/library/asyncio.html) module provides **cooperative multitasking** using `async` / `await` syntax.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "- An `async def` function defines a *coroutine*.\n",
    "- Inside, you use `await` to pause while waiting for an I/O operation (or simulated I/O like `asyncio.sleep`).\n",
    "- The event loop schedules many coroutines concurrently in a single thread.\n",
    "\n",
    "AsyncIO is often a good fit for:\n",
    "\n",
    "- Making many concurrent HTTP requests\n",
    "- Managing many measurement channels over TCP/serial\n",
    "- Any workload that is mostly waiting for I/O\n",
    "\n",
    "It does not bypass the GIL for CPU-bound Python loops, but it can drastically improve throughput for I/O heavy workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0fc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[async] Sensor 4: value=0.9043, delay=0.20s\n",
      "[async] Sensor 2: value=1.0214, delay=0.30s\n",
      "[async] Sensor 1: value=1.0142, delay=0.40s\n",
      "[async] Sensor 3: value=1.0102, delay=0.60s\n",
      "Collected values: ['1.0142', '1.0214', '1.0102', '0.9043']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from random import uniform\n",
    "\n",
    "async def async_measure_sensor(sensor_id: int, delay: float) -> float:\n",
    "    \"\"\"Simulate an async measurement using asyncio.sleep.\"\"\"\n",
    "    await asyncio.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"[async] Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "async def main_async_measurements():\n",
    "    delays = [0.4, 0.3, 0.6, 0.2]\n",
    "    tasks = [asyncio.create_task(async_measure_sensor(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    values = await asyncio.gather(*tasks)\n",
    "    print(\"Collected values:\", [f\"{v:.4f}\" for v in values])\n",
    "\n",
    "await main_async_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3e334",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Async total measurement time\n",
    "\n",
    "Using the `async_measure_sensor` idea:\n",
    "\n",
    "1. Implement a small async program that launches several measurement coroutines with different delays.\n",
    "2. Use `time.perf_counter()` before and after `await asyncio.gather(...)` to measure total time.\n",
    "3. Print the total time.\n",
    "\n",
    "Compare the total time roughly to the maximum individual delay and think about why they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37746201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "async def async_measure_sensor(sensor_id: int, delay: float) -> float:\n",
    "    await asyncio.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "async def main():\n",
    "    delays = [0.4, 0.3, 0.6, 0.2]\n",
    "    tasks = [asyncio.create_task(async_measure_sensor(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    start = time.perf_counter()\n",
    "    # TODO: wait for all tasks and measure total time.\n",
    "    # values = await asyncio.gather(...)\n",
    "    # elapsed = time.perf_counter() - start\n",
    "    # print(f\"Total async elapsed: {elapsed:.3f} s\")\n",
    "    # print(\"Values:\", values)\n",
    "\n",
    "# await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf5f080",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor 4: value=1.0771, delay=0.20s\n",
      "Sensor 2: value=1.0840, delay=0.30s\n",
      "Sensor 1: value=0.9966, delay=0.40s\n",
      "Sensor 3: value=0.9108, delay=0.60s\n",
      "Total async elapsed: 0.606 s\n",
      "Values: ['0.9966', '1.0840', '0.9108', '1.0771']\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "async def async_measure_sensor(sensor_id: int, delay: float) -> float:\n",
    "    await asyncio.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "async def main():\n",
    "    delays = [0.4, 0.3, 0.6, 0.2]\n",
    "    tasks = [asyncio.create_task(async_measure_sensor(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    start = time.perf_counter()\n",
    "    values = await asyncio.gather(*tasks)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Total async elapsed: {elapsed:.3f} s\")\n",
    "    print(\"Values:\", [f\"{v:.4f}\" for v in values])\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef040c",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Async retries for flaky measurements\n",
    "\n",
    "Sometimes a measurement channel fails and you need to retry.\n",
    "\n",
    "1. Implement an `async_measure_with_retry(sensor_id, delay, retries)` coroutine.\n",
    "2. It should call `async_measure_sensor` and catch a simulated error. You can simulate errors by randomly raising an exception in `async_measure_sensor` based on a probability.\n",
    "3. On error, retry up to `retries` times, then re-raise the last exception.\n",
    "4. Run several such tasks in parallel with `asyncio.gather`.\n",
    "\n",
    "This mirrors real measurement systems where some readings occasionally fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from random import uniform, random\n",
    "\n",
    "async def flaky_async_measure_sensor(sensor_id: int, delay: float) -> float:\n",
    "    await asyncio.sleep(delay)\n",
    "    if random() < 0.3:\n",
    "        raise RuntimeError(f\"Sensor {sensor_id} temporary failure\")\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"OK Sensor {sensor_id}: value={value:.4f}\")\n",
    "    return value\n",
    "\n",
    "async def async_measure_with_retry(sensor_id: int, delay: float, retries: int = 3) -> float:\n",
    "    \"\"\"TODO: call flaky_async_measure_sensor, retry up to 'retries' times on error.\"\"\"\n",
    "    # for attempt in range(retries + 1):\n",
    "    #     try:\n",
    "    #         return await flaky_async_measure_sensor(sensor_id, delay)\n",
    "    #     except RuntimeError as e:\n",
    "    #         print(f\"Sensor {sensor_id} failed (attempt {attempt+1}): {e}\")\n",
    "    # raise RuntimeError(f\"Sensor {sensor_id} failed after {retries+1} attempts\")\n",
    "    raise NotImplementedError\n",
    "\n",
    "async def main():\n",
    "    delays = [0.2, 0.3, 0.4]\n",
    "    tasks = [asyncio.create_task(async_measure_with_retry(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    # TODO: await asyncio.gather and handle exceptions.\n",
    "    # results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    # print(results)\n",
    "\n",
    "# await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b64973",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor 1 failed (attempt 1): Sensor 1 temporary failure\n",
      "Sensor 2 failed (attempt 1): Sensor 2 temporary failure\n",
      "OK Sensor 3: value=1.0574\n",
      "OK Sensor 1: value=1.0100\n",
      "Sensor 2 failed (attempt 2): Sensor 2 temporary failure\n",
      "Sensor 2 failed (attempt 3): Sensor 2 temporary failure\n",
      "Sensor 2 failed (attempt 4): Sensor 2 temporary failure\n",
      "Results or exceptions: [1.0099671280246731, RuntimeError('Sensor 2 failed after 4 attempts'), 1.0574018455389373]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import asyncio\n",
    "from random import uniform, random\n",
    "\n",
    "async def flaky_async_measure_sensor(sensor_id: int, delay: float) -> float:\n",
    "    await asyncio.sleep(delay)\n",
    "    if random() < 0.3:\n",
    "        raise RuntimeError(f\"Sensor {sensor_id} temporary failure\")\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"OK Sensor {sensor_id}: value={value:.4f}\")\n",
    "    return value\n",
    "\n",
    "async def async_measure_with_retry(sensor_id: int, delay: float, retries: int = 3) -> float:\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            return await flaky_async_measure_sensor(sensor_id, delay)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Sensor {sensor_id} failed (attempt {attempt+1}): {e}\")\n",
    "    raise RuntimeError(f\"Sensor {sensor_id} failed after {retries+1} attempts\")\n",
    "\n",
    "async def main():\n",
    "    delays = [0.2, 0.3, 0.4]\n",
    "    tasks = [asyncio.create_task(async_measure_with_retry(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    print(\"Results or exceptions:\", results)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c619540",
   "metadata": {},
   "source": [
    "---\n",
    "# Lunch break (12:00 - 13:00)\n",
    "\n",
    "Time to rest your brain and fuel it. We will dive into NumPy and vectorized computations afterwards.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e944fc6",
   "metadata": {},
   "source": [
    "## Topic 5 - NumPy recap and basic vectorization\n",
    "\n",
    "[NumPy](https://numpy.org/) is the standard array library for numerical computing in Python.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "- A `numpy.ndarray` is an efficient, typed, homogeneous n-dimensional array.\n",
    "- Many operations are implemented in optimized C code.\n",
    "- Operations like `a + b`, `a * b` on arrays are **vectorized**: they run in fast loops in C rather than Python.\n",
    "\n",
    "In measurement and physics workflows, NumPy is ideal for:\n",
    "\n",
    "- Handling long arrays of measured values (heights, diameters, thicknesses, voltages).\n",
    "- Computing statistics and transformations (offset correction, unit conversion, normalization).\n",
    "- Applying elementwise functions (e.g. calibration curves, non-linear corrections).\n",
    "\n",
    "Make sure you have NumPy installed. In this course environment it should already be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw thickness (um): [100.2  99.8 100.5 100.1  99.9 100.3 100.   99.7 100.4 100.1]\n",
      "Shape: (10,) dtype: float64\n",
      "Thickness (mm): [0.1002 0.0998 0.1005 0.1001 0.0999 0.1003 0.1    0.0997 0.1004 0.1001]\n",
      "Mean (um): 100.1\n",
      "Std (um): 0.24494897427831783\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Thickness measurements in micrometers for 10 samples\n",
    "thickness_um = np.array([100.2, 99.8, 100.5, 100.1, 99.9, 100.3, 100.0, 99.7, 100.4, 100.1])\n",
    "print(\"Raw thickness (um):\", thickness_um)\n",
    "print(\"Shape:\", thickness_um.shape, \"dtype:\", thickness_um.dtype)\n",
    "\n",
    "# Convert to millimeters\n",
    "thickness_mm = thickness_um / 1000.0\n",
    "print(\"Thickness (mm):\", thickness_mm)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Mean (um):\", thickness_um.mean())\n",
    "print(\"Std (um):\", thickness_um.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449ce64",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Diameter conversion and simple statistics\n",
    "\n",
    "1. Create a NumPy array of diameters in millimeters for at least 8 parts.\n",
    "2. Convert them to micrometers (multiply by 1000).\n",
    "3. Compute and print the mean and standard deviation in micrometers.\n",
    "\n",
    "Use the same patterns as in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: diameter conversion and basic statistics with NumPy.\n",
    "\n",
    "# diam_mm = np.array([...])\n",
    "# diam_um = ...\n",
    "# mean_um = ...\n",
    "# std_um = ...\n",
    "# print(\"Diameters (um):\", diam_um)\n",
    "# print(\"Mean (um):\", mean_um)\n",
    "# print(\"Std (um):\", std_um)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64f6237",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diameters (um): [10010.  9990. 10020. 10000. 10030.  9980. 10010.  9970.]\n",
      "Mean (um): 10001.25\n",
      "Std (um): 18.99835519196333\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "diam_mm = np.array([10.01, 9.99, 10.02, 10.00, 10.03, 9.98, 10.01, 9.97])\n",
    "diam_um = diam_mm * 1000.0\n",
    "mean_um = diam_um.mean()\n",
    "std_um = diam_um.std()\n",
    "print(\"Diameters (um):\", diam_um)\n",
    "print(\"Mean (um):\", mean_um)\n",
    "print(\"Std (um):\", std_um)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd42de",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Offset correction and normalized values\n",
    "\n",
    "Imagine your thickness sensor has a calibration offset error of +0.3 micrometers.\n",
    "\n",
    "1. Create an array of measured thicknesses in micrometers.\n",
    "2. Subtract the offset from all values to get corrected thicknesses.\n",
    "3. Compute the mean and standard deviation of the corrected values.\n",
    "4. Compute a normalized array where you subtract the mean and divide by the standard deviation.\n",
    "\n",
    "This pattern (offset correction + normalization) is common in data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe711695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: offset correction and normalization.\n",
    "# thickness_um = np.array([...])\n",
    "# offset = 0.3\n",
    "# corrected = ...  # subtract offset\n",
    "# mean_corr = ...\n",
    "# std_corr = ...\n",
    "# normalized = ...  # (corrected - mean_corr) / std_corr\n",
    "# print(\"Corrected thickness (um):\", corrected)\n",
    "# print(\"Normalized values:\", normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6355e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected thickness (um): [ 99.9  99.7  99.6 100.1  99.8  99.5]\n",
      "Normalized values: [ 0.6761234  -0.3380617  -0.84515425  1.69030851  0.16903085 -1.35224681]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "thickness_um = np.array([100.2, 100.0, 99.9, 100.4, 100.1, 99.8])\n",
    "offset = 0.3\n",
    "corrected = thickness_um - offset\n",
    "mean_corr = corrected.mean()\n",
    "std_corr = corrected.std()\n",
    "normalized = (corrected - mean_corr) / std_corr\n",
    "print(\"Corrected thickness (um):\", corrected)\n",
    "print(\"Normalized values:\", normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e7649",
   "metadata": {},
   "source": [
    "## Topic 6 - NumPy vectorized computations in practice\n",
    "\n",
    "Now we go deeper into NumPy vectorization. We will use:\n",
    "\n",
    "- Elementwise operations on arrays\n",
    "- Boolean masks and filtering\n",
    "- Aggregations along axes (2D arrays)\n",
    "- Simple physics-themed computations\n",
    "\n",
    "NumPy allows you to write **array expressions** instead of Python `for` loops. These expressions are executed in optimized C loops under the hood.\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- NumPy user guide: https://numpy.org/doc/stable/user/index.html\n",
    "- NumPy quickstart: https://numpy.org/doc/stable/user/quickstart.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759bf60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diameters: [10.01  9.97 10.05 10.02  9.94 10.    9.99]\n",
      "Deviation: [ 0.01 -0.03  0.05  0.02 -0.06  0.   -0.01]\n",
      "In spec mask: [ True  True False  True False  True  True]\n",
      "In spec diameters: [10.01  9.97 10.02 10.    9.99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: filter diameters by tolerance\n",
    "diam_mm = np.array([10.01, 9.97, 10.05, 10.02, 9.94, 10.00, 9.99])\n",
    "target = 10.00\n",
    "tolerance = 0.03\n",
    "\n",
    "deviation = diam_mm - target\n",
    "mask_in_spec = np.abs(deviation) <= tolerance\n",
    "print(\"Diameters:\", diam_mm)\n",
    "print(\"Deviation:\", deviation)\n",
    "print(\"In spec mask:\", mask_in_spec)\n",
    "print(\"In spec diameters:\", diam_mm[mask_in_spec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28be655",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Filter out-of-spec samples\n",
    "\n",
    "1. Create a NumPy array of length measurements in millimeters.\n",
    "2. Define a target and tolerance.\n",
    "3. Build a mask of samples that are **out of spec** (absolute deviation larger than tolerance).\n",
    "4. Print the array of out-of-spec values and their count.\n",
    "\n",
    "Use boolean masks like in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: filter out-of-spec length values using a boolean mask.\n",
    "\n",
    "# lengths_mm = np.array([...])\n",
    "# target = ...\n",
    "# tolerance = ...\n",
    "# deviation = ...\n",
    "# mask_out = ...  # abs(deviation) > tolerance\n",
    "# print(\"Out-of-spec values:\", lengths_mm[mask_out])\n",
    "# print(\"Count:\", mask_out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1d642e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-spec values: [49.95 50.04 49.92]\n",
      "Count: 3\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lengths_mm = np.array([49.98, 50.02, 49.95, 50.04, 50.01, 49.92])\n",
    "target = 50.00\n",
    "tolerance = 0.03\n",
    "deviation = lengths_mm - target\n",
    "mask_out = np.abs(deviation) > tolerance\n",
    "print(\"Out-of-spec values:\", lengths_mm[mask_out])\n",
    "print(\"Count:\", int(mask_out.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ecefb",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Repeated measurements per part (2D arrays)\n",
    "\n",
    "Suppose you measure the same part multiple times to estimate uncertainty.\n",
    "\n",
    "1. Create a 2D NumPy array `data` of shape `(n_parts, n_repeats)` containing diameters in mm.\n",
    "2. Compute the mean diameter **per part** (axis 1).\n",
    "3. Compute the standard deviation per part (axis 1).\n",
    "4. Compute the overall mean of all measurements.\n",
    "5. Print the per-part mean and standard deviation.\n",
    "\n",
    "You should use `data.mean(axis=1)` and `data.std(axis=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: repeated measurement statistics with 2D arrays.\n",
    "\n",
    "# Example: 4 parts, 5 repeated measurements each\n",
    "# data = np.array([\n",
    "#     [...],\n",
    "#     [...],\n",
    "# ])\n",
    "\n",
    "# mean_per_part = ...  # data.mean(axis=1)\n",
    "# std_per_part = ...   # data.std(axis=1)\n",
    "# overall_mean = ...   # data.mean()\n",
    "\n",
    "# print(\"Mean per part:\", mean_per_part)\n",
    "# print(\"Std per part:\", std_per_part)\n",
    "# print(\"Overall mean:\", overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e901fac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean per part: [10.  5. 20. 30.]\n",
      "Std per part: [0.01414214 0.01414214 0.01414214 0.01414214]\n",
      "Overall mean: 16.25\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([\n",
    "    [10.01, 9.99, 10.00, 10.02, 9.98],\n",
    "    [4.99, 5.01, 5.00, 5.02, 4.98],\n",
    "    [19.98, 20.00, 20.01, 19.99, 20.02],\n",
    "    [29.99, 30.01, 30.00, 30.02, 29.98],\n",
    "])\n",
    "\n",
    "mean_per_part = data.mean(axis=1)\n",
    "std_per_part = data.std(axis=1)\n",
    "overall_mean = data.mean()\n",
    "\n",
    "print(\"Mean per part:\", mean_per_part)\n",
    "print(\"Std per part:\", std_per_part)\n",
    "print(\"Overall mean:\", overall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ae6db",
   "metadata": {},
   "source": [
    "### Vectorized physics-style computation example\n",
    "\n",
    "As a small example, imagine you measured heights `h` at given positions `x` along a line, and you want to approximate the area under the curve using the trapezoidal rule.\n",
    "\n",
    "The trapezoidal rule for arrays `x` and `h` can be written as:\n",
    "\n",
    "`area ‚âà sum( (h[i] + h[i+1]) / 2 * (x[i+1] - x[i]) )`\n",
    "\n",
    "We can implement this with pure NumPy, using slicing, without explicit Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd2100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate area (um*mm): 100.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate positions in mm and heights in micrometers\n",
    "x = np.linspace(0.0, 10.0, 1001)  # 0..10 mm, 1001 points\n",
    "h_um = 2.0 * np.sin(2 * np.pi * x / 10.0) + 10.0  # some periodic height pattern in um\n",
    "\n",
    "# Trapezoidal rule using vectorized slices\n",
    "dx = x[1:] - x[:-1]\n",
    "h_avg = (h_um[1:] + h_um[:-1]) / 2.0\n",
    "area_um_mm = np.sum(h_avg * dx)  # units: um * mm\n",
    "\n",
    "print(f\"Approximate area (um*mm): {area_um_mm:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87a572",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced - optional): Chain of vectorized operations\n",
    "\n",
    "Combine several vectorized operations into a small pipeline:\n",
    "\n",
    "1. Simulate an array of thicknesses in micrometers with `np.random.normal`.\n",
    "2. Apply an offset correction.\n",
    "3. Clip the values to a realistic range using `np.clip`.\n",
    "4. Convert to millimeters.\n",
    "5. Compute and print the mean and standard deviation in both units.\n",
    "\n",
    "Do not use explicit Python loops. Use NumPy array operations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31633097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: implement the vectorized processing pipeline.\n",
    "\n",
    "# n = 1000\n",
    "# thickness_um = np.random.normal(loc=100.0, scale=0.5, size=n)\n",
    "# offset = 0.2\n",
    "# corrected = ...\n",
    "# clipped = ...  # np.clip\n",
    "# thickness_mm = ...\n",
    "# print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4c592d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (um): 99.80521127498606\n",
      "Std (um): 0.5115783054404365\n",
      "Mean (mm): 0.09980521127498607\n",
      "Std (mm): 0.0005115783054404363\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "thickness_um = np.random.normal(loc=100.0, scale=0.5, size=n)\n",
    "offset = 0.2\n",
    "corrected = thickness_um - offset\n",
    "clipped = np.clip(corrected, 98.0, 102.0)\n",
    "thickness_mm = clipped / 1000.0\n",
    "\n",
    "print(\"Mean (um):\", clipped.mean())\n",
    "print(\"Std (um):\", clipped.std())\n",
    "print(\"Mean (mm):\", thickness_mm.mean())\n",
    "print(\"Std (mm):\", thickness_mm.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8572f",
   "metadata": {},
   "source": [
    "---\n",
    "# Short break (14:45 - 15:00)\n",
    "\n",
    "Final stretch: Numba, GPUs, Python 3.13, and a complex example.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9c68c",
   "metadata": {},
   "source": [
    "## Topic 7 - Numba JIT compilation\n",
    "\n",
    "[Numba](https://numba.pydata.org/) is a Just-In-Time (JIT) compiler for Python functions that operate mainly on NumPy arrays and numbers.\n",
    "\n",
    "Basic usage:\n",
    "\n",
    "```python\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def f(x):\n",
    "    # numerical code\n",
    "    ...\n",
    "```\n",
    "\n",
    "When you first call `f`, Numba compiles it to machine code (using LLVM). Subsequent calls run at near-C speed.\n",
    "\n",
    "### Important: how Numba and NumPy interact\n",
    "\n",
    "- NumPy operations like `a + b` or `a.mean()` are already implemented in C.\n",
    "- However, **Python loops around those operations** still run in the Python interpreter.\n",
    "- Numba helps most when you have **custom loops and logic** that cannot be expressed as simple NumPy expressions.\n",
    "\n",
    "**Answering the question:** \"If Numba works with NumPy + Python code, and NumPy is already implemented in C, how does Numba JIT help?\"\n",
    "\n",
    "- NumPy is fast for each individual operation, but if you write Python like:\n",
    "  - `for i in range(n): result[i] = complex_expression(a[i], b[i])`\n",
    "  - this loop runs in Python and pays Python overhead per iteration.\n",
    "- Numba compiles the **whole loop** (and the operations inside it) into one optimized machine code function.\n",
    "- This removes Python overhead and can fuse several operations into one pass.\n",
    "\n",
    "In practice, combine them:\n",
    "\n",
    "- Use NumPy vectorization where it is natural.\n",
    "- Use Numba for custom numeric kernels that are hard to write as a single NumPy expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f86e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python RMS: 1.000860, time=0.0106 s\n",
      "Numba RMS first call: 1.000860, time=1.4904 s (includes compile)\n",
      "Numba RMS second call: 1.000860, time=0.0001 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba is not installed in this environment. The examples will fall back to pure Python.\")\n",
    "\n",
    "def rms_python(arr: np.ndarray) -> float:\n",
    "    s = 0.0\n",
    "    n = arr.size\n",
    "    for i in range(n):\n",
    "        x = float(arr[i])\n",
    "        s += x * x\n",
    "    return math.sqrt(s / n)\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def rms_numba(arr):\n",
    "        s = 0.0\n",
    "        n = arr.size\n",
    "        for i in range(n):\n",
    "            x = arr[i]\n",
    "            s += x * x\n",
    "        return math.sqrt(s / n)\n",
    "else:\n",
    "    rms_numba = None\n",
    "\n",
    "# Test on a large array\n",
    "arr = np.random.normal(loc=0.0, scale=1.0, size=1_000_00)\n",
    "\n",
    "# Python version\n",
    "start = time.perf_counter()\n",
    "r1 = rms_python(arr)\n",
    "t_python = time.perf_counter() - start\n",
    "print(f\"Python RMS: {r1:.6f}, time={t_python:.4f} s\")\n",
    "\n",
    "if rms_numba is not None:\n",
    "    # First call includes compilation time\n",
    "    start = time.perf_counter()\n",
    "    r2 = rms_numba(arr)\n",
    "    t_first = time.perf_counter() - start\n",
    "    # Second call is fast\n",
    "    start = time.perf_counter()\n",
    "    r3 = rms_numba(arr)\n",
    "    t_numba = time.perf_counter() - start\n",
    "    print(f\"Numba RMS first call: {r2:.6f}, time={t_first:.4f} s (includes compile)\")\n",
    "    print(f\"Numba RMS second call: {r3:.6f}, time={t_numba:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3c3f3",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Numba-accelerated difference of squares\n",
    "\n",
    "1. Implement a function `diff_squares_python(a, b)` that for each element computes `a[i]**2 - b[i]**2`.\n",
    "2. Time it on large NumPy arrays.\n",
    "3. If Numba is available, implement `diff_squares_numba` with `@njit`.\n",
    "4. Compare the timings.\n",
    "\n",
    "Make sure you do not create new Python lists inside the function. Work directly with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb29f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - you can still implement the pure Python version.\")\n",
    "\n",
    "def diff_squares_python(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    n = a.size\n",
    "    out = np.empty_like(a)\n",
    "    for i in range(n):\n",
    "        out[i] = a[i] * a[i] - b[i] * b[i]\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def diff_squares_numba(a, b):\n",
    "        n = a.size\n",
    "        out = np.empty_like(a)\n",
    "        for i in range(n):\n",
    "            out[i] = a[i] * a[i] - b[i] * b[i]\n",
    "        return out\n",
    "\n",
    "# a = np.random.normal(size=200_000)\n",
    "# b = np.random.normal(size=200_000)\n",
    "# start = time.perf_counter()\n",
    "# out_py = diff_squares_python(a, b)\n",
    "# t_py = time.perf_counter() - start\n",
    "# print(f\"Python time: {t_py:.4f} s\")\n",
    "\n",
    "# if njit is not None:\n",
    "#     start = time.perf_counter()\n",
    "#     out_nb1 = diff_squares_numba(a, b)\n",
    "#     t_nb1 = time.perf_counter() - start\n",
    "#     start = time.perf_counter()\n",
    "#     out_nb2 = diff_squares_numba(a, b)\n",
    "#     t_nb2 = time.perf_counter() - start\n",
    "#     print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7881d0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python time: 0.1077 s\n",
      "Numba first call: 0.3910 s, second call: 0.0004 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - only Python version will run.\")\n",
    "\n",
    "def diff_squares_python(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    n = a.size\n",
    "    out = np.empty_like(a)\n",
    "    for i in range(n):\n",
    "        out[i] = a[i] * a[i] - b[i] * b[i]\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def diff_squares_numba(a, b):\n",
    "        n = a.size\n",
    "        out = np.empty_like(a)\n",
    "        for i in range(n):\n",
    "            out[i] = a[i] * a[i] - b[i] * b[i]\n",
    "        return out\n",
    "\n",
    "a = np.random.normal(size=200_000)\n",
    "b = np.random.normal(size=200_000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "out_py = diff_squares_python(a, b)\n",
    "t_py = time.perf_counter() - start\n",
    "print(f\"Python time: {t_py:.4f} s\")\n",
    "\n",
    "if njit is not None:\n",
    "    start = time.perf_counter()\n",
    "    out_nb1 = diff_squares_numba(a, b)\n",
    "    t_nb1 = time.perf_counter() - start\n",
    "    start = time.perf_counter()\n",
    "    out_nb2 = diff_squares_numba(a, b)\n",
    "    t_nb2 = time.perf_counter() - start\n",
    "    print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de043d52",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Custom statistic with Numba\n",
    "\n",
    "Define a custom function that is harder to express with pure NumPy:\n",
    "\n",
    "1. Implement `moving_rms_python(arr, window)` that computes RMS roughness over a sliding window.\n",
    "   - For each position `i`, compute RMS of `arr[i : i+window]`.\n",
    "2. If Numba is available, implement `moving_rms_numba` using `@njit`.\n",
    "3. Compare performance on a large array.\n",
    "\n",
    "This is a common pattern when analyzing profiles from surface measurement devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - you can still implement the Python version.\")\n",
    "\n",
    "def moving_rms_python(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    \"\"\"TODO: pure Python moving RMS.\"\"\"\n",
    "    n = arr.size\n",
    "    out = np.empty(n - window + 1, dtype=float)\n",
    "    for i in range(n - window + 1):\n",
    "        s = 0.0\n",
    "        for j in range(window):\n",
    "            x = float(arr[i + j])\n",
    "            s += x * x\n",
    "        out[i] = math.sqrt(s / window)\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def moving_rms_numba(arr, window):\n",
    "        n = arr.size\n",
    "        out = np.empty(n - window + 1, dtype=np.float64)\n",
    "        for i in range(n - window + 1):\n",
    "            s = 0.0\n",
    "            for j in range(window):\n",
    "                x = arr[i + j]\n",
    "                s += x * x\n",
    "            out[i] = math.sqrt(s / window)\n",
    "        return out\n",
    "\n",
    "# arr = np.random.normal(size=200_000)\n",
    "# window = 50\n",
    "# start = time.perf_counter()\n",
    "# r_py = moving_rms_python(arr, window)\n",
    "# t_py = time.perf_counter() - start\n",
    "# print(f\"Python moving RMS time: {t_py:.4f} s\")\n",
    "\n",
    "# if njit is not None:\n",
    "#     start = time.perf_counter()\n",
    "#     r_nb1 = moving_rms_numba(arr, window)\n",
    "#     t_nb1 = time.perf_counter() - start\n",
    "#     start = time.perf_counter()\n",
    "#     r_nb2 = moving_rms_numba(arr, window)\n",
    "#     t_nb2 = time.perf_counter() - start\n",
    "#     print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f197590",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python moving RMS time: 1.3702 s\n",
      "Numba first call: 0.2038 s, second call: 0.0066 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - only Python version will run.\")\n",
    "\n",
    "def moving_rms_python(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    n = arr.size\n",
    "    out = np.empty(n - window + 1, dtype=float)\n",
    "    for i in range(n - window + 1):\n",
    "        s = 0.0\n",
    "        for j in range(window):\n",
    "            x = float(arr[i + j])\n",
    "            s += x * x\n",
    "        out[i] = math.sqrt(s / window)\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def moving_rms_numba(arr, window):\n",
    "        n = arr.size\n",
    "        out = np.empty(n - window + 1, dtype=np.float64)\n",
    "        for i in range(n - window + 1):\n",
    "            s = 0.0\n",
    "            for j in range(window):\n",
    "                x = arr[i + j]\n",
    "                s += x * x\n",
    "            out[i] = math.sqrt(s / window)\n",
    "        return out\n",
    "\n",
    "arr = np.random.normal(size=200_000)\n",
    "window = 50\n",
    "\n",
    "start = time.perf_counter()\n",
    "r_py = moving_rms_python(arr, window)\n",
    "t_py = time.perf_counter() - start\n",
    "print(f\"Python moving RMS time: {t_py:.4f} s\")\n",
    "\n",
    "if njit is not None:\n",
    "    start = time.perf_counter()\n",
    "    r_nb1 = moving_rms_numba(arr, window)\n",
    "    t_nb1 = time.perf_counter() - start\n",
    "    start = time.perf_counter()\n",
    "    r_nb2 = moving_rms_numba(arr, window)\n",
    "    t_nb2 = time.perf_counter() - start\n",
    "    print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51ab2b",
   "metadata": {},
   "source": [
    "## Topic 8 - GPU acceleration overview: cuDF and CuPy\n",
    "\n",
    "For very large datasets and heavy numerical work, GPUs can be useful.\n",
    "\n",
    "Two popular libraries in the Python ecosystem:\n",
    "\n",
    "- [CuPy](https://cupy.dev/):\n",
    "  - NumPy-like interface for arrays stored on a CUDA GPU.\n",
    "  - Many functions mirror the NumPy API (`cupy.array`, `cupy.mean`, etc.).\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/):\n",
    "  - Part of the RAPIDS ecosystem: https://rapids.ai/\n",
    "  - Pandas-like DataFrame library running on the GPU.\n",
    "\n",
    "In many cases, the workflow is:\n",
    "\n",
    "- Move large arrays or tables to the GPU once.\n",
    "- Perform many operations there.\n",
    "- Move reduced results (e.g. aggregates) back to the CPU.\n",
    "\n",
    "In this course environment, GPUs might not be available, so the examples below are illustrative only. Do not worry if they raise `ImportError` - that is expected on a CPU-only machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba512f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy is not installed. This example is for illustration only.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import cupy as cp\n",
    "    print(\"CuPy version:\", cp.__version__)\n",
    "\n",
    "    # Create an array on the GPU\n",
    "    a_gpu = cp.random.normal(size=1_000_00)\n",
    "    mean_gpu = a_gpu.mean()\n",
    "    std_gpu = a_gpu.std()\n",
    "    print(\"GPU mean:\", float(mean_gpu), \"GPU std:\", float(std_gpu))\n",
    "except ImportError:\n",
    "    print(\"CuPy is not installed. This example is for illustration only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf814b48",
   "metadata": {},
   "source": [
    "## Topic 9 - Python 3.13 and the future: experimental JIT and optional GIL\n",
    "\n",
    "Recent and upcoming CPython releases are adding major performance features:\n",
    "\n",
    "- **Python 3.13** (released in 2024) includes:\n",
    "  - An experimental **free-threaded build** (optional no-GIL mode). See PEP 703.\n",
    "  - An experimental **JIT compiler** (PEP 744) that can speed up some workloads.\n",
    "  - These features are **off by default** and require special builds / flags.\n",
    "- Future versions (3.14 and beyond) are expected to improve JIT performance and evolve the no-GIL story.\n",
    "\n",
    "What this means for you in the medium term:\n",
    "\n",
    "- Well-written numeric Python might become faster without you changing code.\n",
    "- True multi-threaded CPU-bound Python code may become possible without having to use `multiprocessing`.\n",
    "- Libraries like Numba, cuDF, CuPy, and others will likely evolve to take advantage of new capabilities.\n",
    "\n",
    "Official resources:\n",
    "\n",
    "- What's new in Python 3.13: https://docs.python.org/3/whatsnew/3.13.html\n",
    "- PEP 703 (optional GIL): https://peps.python.org/pep-0703/\n",
    "- PEP 744 (JIT): https://peps.python.org/pep-0744/\n",
    "\n",
    "For now, you should still learn threads, processes, AsyncIO, NumPy, and Numba - these skills remain valuable regardless of how the interpreter evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d7fb711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python version: 3.13.7 (main, Sep  2 2025, 14:16:00) [MSC v.1944 64 bit (AMD64)]\n",
      "Executable: C:\\Users\\gregk\\Desktop\\winpython\\WPy64-31700\\python\\python.exe\n",
      "Note: in standard CPython 3.13+, JIT and no-GIL builds are optional and may need explicit enabling.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Running Python version:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Note: in standard CPython 3.13+, JIT and no-GIL builds are optional and may need explicit enabling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5823c",
   "metadata": {},
   "source": [
    "## Topic 10 - Complex example: parallel processing of measurement datasets\n",
    "\n",
    "In this final example we combine multiple ideas from today:\n",
    "\n",
    "- NumPy arrays and vectorized computations\n",
    "- Numba JIT for a custom numeric kernel (optional)\n",
    "- `multiprocessing` to process multiple independent datasets in parallel\n",
    "\n",
    "### Scenario\n",
    "\n",
    "You have several measurement files from a surface profiler. Each file contains a 1D height profile (in micrometers). For each profile, you want to:\n",
    "\n",
    "1. Load the data (in this notebook we will just simulate it).\n",
    "2. Apply an offset correction (subtract mean).\n",
    "3. Compute RMS roughness and peak-to-valley height (max - min).\n",
    "4. Return a small summary dictionary.\n",
    "\n",
    "Then, for many profiles (e.g. 8 or 16), you want to process them in parallel using multiple CPU cores.\n",
    "\n",
    "We will build:\n",
    "\n",
    "- A pure NumPy summary function.\n",
    "- Optionally a Numba-accelerated variant.\n",
    "- A small wrapper that can be used with `multiprocessing.Pool.map`.\n",
    "\n",
    "Your task is to fill in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "\n",
    "def simulate_profile(n_points: int = 50_000) -> np.ndarray:\n",
    "    \"\"\"Simulate a 1D surface profile in micrometers.\"\"\"\n",
    "    x = np.linspace(0.0, 10.0, n_points)\n",
    "    base = 5.0 * np.sin(2 * np.pi * x / 5.0)\n",
    "    noise = np.random.normal(loc=0.0, scale=0.5, size=n_points)\n",
    "    return base + noise\n",
    "\n",
    "def summarize_profile_numpy(profile: np.ndarray) -> dict:\n",
    "    \"\"\"TODO: center profile and compute RMS and peak-to-valley using NumPy only.\"\"\"\n",
    "    # mean = ...\n",
    "    # centered = ...\n",
    "    # rms = ...\n",
    "    # ptv = ...\n",
    "    # return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": float(ptv)}\n",
    "    mean = profile.mean()\n",
    "    centered = profile - mean\n",
    "    rms = math.sqrt((centered * centered).mean())\n",
    "    ptv = float(centered.max() - centered.min())\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": ptv}\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def summarize_profile_numba_kernel(profile):\n",
    "        n = profile.size\n",
    "        # Compute mean\n",
    "        s = 0.0\n",
    "        for i in range(n):\n",
    "            s += profile[i]\n",
    "        mean = s / n\n",
    "        # Compute RMS and min, max of centered profile\n",
    "        s2 = 0.0\n",
    "        min_c = 1e30\n",
    "        max_c = -1e30\n",
    "        for i in range(n):\n",
    "            c = profile[i] - mean\n",
    "            s2 += c * c\n",
    "            if c < min_c:\n",
    "                min_c = c\n",
    "            if c > max_c:\n",
    "                max_c = c\n",
    "        rms = math.sqrt(s2 / n)\n",
    "        ptv = max_c - min_c\n",
    "        return mean, rms, ptv\n",
    "\n",
    "def summarize_profile_numba(profile: np.ndarray) -> dict:\n",
    "    if njit is None:\n",
    "        return summarize_profile_numpy(profile)\n",
    "    mean, rms, ptv = summarize_profile_numba_kernel(profile)\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": float(ptv)}\n",
    "\n",
    "def process_one_profile(args):\n",
    "    \"\"\"Wrapper for Pool.map: args could be (index, use_numba).\"\"\"\n",
    "    index, use_numba = args\n",
    "    profile = simulate_profile()\n",
    "    if use_numba:\n",
    "        summary = summarize_profile_numba(profile)\n",
    "    else:\n",
    "        summary = summarize_profile_numpy(profile)\n",
    "    summary[\"index\"] = index\n",
    "    return summary\n",
    "\n",
    "# TODO:\n",
    "# 1) Create a list of indices, e.g. range(8)\n",
    "# 2) Use Pool to process them in parallel\n",
    "# 3) Print the summaries sorted by index\n",
    "\n",
    "# indices = ...\n",
    "# args_list = [(i, True) for i in indices]\n",
    "# with Pool() as pool:\n",
    "#     results = pool.map(process_one_profile, args_list)\n",
    "# results_sorted = sorted(results, key=lambda d: d[\"index\"])\n",
    "# for r in results_sorted:\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97374360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "\n",
    "def simulate_profile(n_points: int = 50_000) -> np.ndarray:\n",
    "    x = np.linspace(0.0, 10.0, n_points)\n",
    "    base = 5.0 * np.sin(2 * np.pi * x / 5.0)\n",
    "    noise = np.random.normal(loc=0.0, scale=0.5, size=n_points)\n",
    "    return base + noise\n",
    "\n",
    "def summarize_profile_numpy(profile: np.ndarray) -> dict:\n",
    "    mean = profile.mean()\n",
    "    centered = profile - mean\n",
    "    rms = math.sqrt((centered * centered).mean())\n",
    "    ptv = float(centered.max() - centered.min())\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": ptv}\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def summarize_profile_numba_kernel(profile):\n",
    "        n = profile.size\n",
    "        s = 0.0\n",
    "        for i in range(n):\n",
    "            s += profile[i]\n",
    "        mean = s / n\n",
    "        s2 = 0.0\n",
    "        min_c = 1e30\n",
    "        max_c = -1e30\n",
    "        for i in range(n):\n",
    "            c = profile[i] - mean\n",
    "            s2 += c * c\n",
    "            if c < min_c:\n",
    "                min_c = c\n",
    "            if c > max_c:\n",
    "                max_c = c\n",
    "        rms = math.sqrt(s2 / n)\n",
    "        ptv = max_c - min_c\n",
    "        return mean, rms, ptv\n",
    "\n",
    "def summarize_profile_numba(profile: np.ndarray) -> dict:\n",
    "    if njit is None:\n",
    "        return summarize_profile_numpy(profile)\n",
    "    mean, rms, ptv = summarize_profile_numba_kernel(profile)\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": float(ptv)}\n",
    "\n",
    "def process_one_profile(args):\n",
    "    index, use_numba = args\n",
    "    profile = simulate_profile()\n",
    "    if use_numba:\n",
    "        summary = summarize_profile_numba(profile)\n",
    "    else:\n",
    "        summary = summarize_profile_numpy(profile)\n",
    "    summary[\"index\"] = index\n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    indices = list(range(8))\n",
    "    args_list = [(i, True) for i in indices]\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_one_profile, args_list)\n",
    "    results_sorted = sorted(results, key=lambda d: d[\"index\"])\n",
    "    for r in results_sorted:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e1a87",
   "metadata": {},
   "source": [
    "## Day 3 summary\n",
    "\n",
    "Today you:\n",
    "\n",
    "- Built a mental model of **CPU-bound** vs **I/O-bound** workloads.\n",
    "- Reviewed the impact of the **GIL** on threads and why processes are used for CPU-bound speedups.\n",
    "- Used **threads** for concurrent I/O-style tasks, collecting results safely with locks.\n",
    "- Used **multiprocessing** to process independent batches of measurement data in parallel.\n",
    "- Learned the basics of **AsyncIO** and coordinated multiple async measurement coroutines.\n",
    "- Revisited **NumPy**, created arrays, applied vectorized transformations, and computed statistics for physics/measurement data.\n",
    "- Practiced boolean masks, axis-wise aggregations, and small vectorized physics-style calculations.\n",
    "- Used **Numba** to JIT-compile custom numeric kernels and understood how it complements NumPy.\n",
    "- Saw an overview of **GPU tools** like [CuPy](https://cupy.dev/) and [cuDF](https://docs.rapids.ai/api/cudf/stable/) for GPU acceleration.\n",
    "- Discussed **Python 3.13** and its experimental JIT and optional no-GIL builds, and how future CPython versions may affect performance.\n",
    "- Combined multiple concepts in a complex example: parallel processing of simulated surface profiles with NumPy, Numba, and multiprocessing.\n",
    "\n",
    "These tools and concepts form a practical toolbox for high-performance numerical work in Python, especially in physics and engineering contexts. On the next days you can build on this knowledge for more advanced machine learning and deep learning workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fabb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
