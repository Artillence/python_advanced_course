{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c93aa83",
   "metadata": {},
   "source": [
    "# Day 3 - Parallel and High-Performance Python: Threads, AsyncIO, NumPy, Numba, GPUs\n",
    "\n",
    "Welcome to Day 3 of the advanced Python course. Today we focus on **performance** and **parallelism**.\n",
    "\n",
    "We will connect Python language features with how modern CPUs and GPUs execute code. The examples are oriented around physics and mechanical **size/length measurement** data (surface roughness, thickness, diameter measurements, repeated measurements, etc.).\n",
    "\n",
    "## What we will cover today\n",
    "\n",
    "- CPU vs I/O bound work and a high level mental model of performance\n",
    "- The Global Interpreter Lock (GIL) and why it matters\n",
    "- Threads, processes, and when to use which\n",
    "- AsyncIO for I/O-bound tasks (simulated sensor queries)\n",
    "- NumPy vectorized computations for numerical work\n",
    "- Numba JIT compilation for accelerating pure Python loops\n",
    "- A short overview of GPU tools: **cuDF**, **CuPy** and where they fit\n",
    "- A look at Python 3.13 and the future: experimental JIT and optional GIL\n",
    "- A complex end-to-end example: processing multiple measurement files in parallel\n",
    "\n",
    "## Daily agenda and course flow\n",
    "\n",
    "**09:00 - 10:30 (1h 30m)**  \n",
    "- CPU vs I/O bound tasks, performance model\n",
    "- GIL recap and impact on threading\n",
    "- Threads for I/O-bound workloads\n",
    "\n",
    "**10:30 - 10:45 (15m)**  \n",
    "- Short break\n",
    "\n",
    "**10:45 - 12:00 (1h 15m)**  \n",
    "- Processes for CPU-bound workloads\n",
    "- Intro to AsyncIO and simulated asynchronous measurements\n",
    "\n",
    "**12:00 - 13:00 (1h)**  \n",
    "- Lunch break\n",
    "\n",
    "**13:00 - 14:45 (1h 45m)**  \n",
    "- NumPy recap and vectorized computations for measurement data\n",
    "- Practical NumPy vectorization patterns and exercises (physics themed)\n",
    "\n",
    "**14:45 - 15:00 (15m)**  \n",
    "- Short break\n",
    "\n",
    "**15:00 - 16:30 (1h 30m)**  \n",
    "- Numba JIT compilation for numerical loops\n",
    "- Overview of GPU tools (cuDF, CuPy) and limitations\n",
    "- Python 3.13: experimental JIT and optional GIL, and what it may mean\n",
    "- Complex example: parallel processing of multiple measurement data sets\n",
    "\n",
    "Throughout the day, we will mark good points to pause, ask questions, or take a sip of water. Try to roughly follow the timing so that we finish comfortably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685d83a",
   "metadata": {},
   "source": [
    "## Topic 1 - Performance mental model: CPU vs I/O, GIL recap\n",
    "\n",
    "In real scientific and engineering work, **performance** usually means one of:\n",
    "\n",
    "- How fast we can process a large dataset (throughput)\n",
    "- How fast we get a single answer (latency)\n",
    "\n",
    "Python performance is strongly influenced by:\n",
    "\n",
    "- The speed of the underlying CPU or GPU\n",
    "- Whether our code is **CPU-bound** or **I/O-bound**\n",
    "- How much work we keep in **C-accelerated libraries** like NumPy\n",
    "- The **Global Interpreter Lock (GIL)** in CPython\n",
    "\n",
    "### CPU-bound vs I/O-bound\n",
    "\n",
    "- **CPU-bound**: most time is spent doing computations on the CPU.\n",
    "  - Example: computing statistics on millions of surface height samples.\n",
    "- **I/O-bound**: most time is spent waiting for input/output.\n",
    "  - Example: reading files from disk or waiting for a measurement device over the network.\n",
    "\n",
    "### GIL recap\n",
    "\n",
    "The CPython interpreter uses a **Global Interpreter Lock (GIL)**. Only one thread at a time can execute Python bytecode. This means:\n",
    "\n",
    "- Multiple threads do **not** speed up CPU-bound pure Python code.\n",
    "- Threads can still help with I/O-bound tasks, because while one thread waits for I/O, another can run.\n",
    "- Libraries that release the GIL internally (NumPy, some C extensions) can still run in parallel in C.\n",
    "\n",
    "For a deeper dive, see:\n",
    "\n",
    "- CPython GIL overview: https://wiki.python.org/moin/GlobalInterpreterLock\n",
    "- CPython implementation notes: https://docs.python.org/3/c-api/init.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ef409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU-bound function took ~0.131 s\n",
      "I/O-bound function took ~0.202 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def cpu_bound(n: int) -> int:\n",
    "    \"\"\"Fake CPU work: sum of squares up to n.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "def io_bound(delay: float) -> None:\n",
    "    \"\"\"Fake I/O: sleep to simulate waiting for a device or disk.\"\"\"\n",
    "    time.sleep(delay)\n",
    "\n",
    "start = time.perf_counter()\n",
    "cpu_bound(2_000_000)\n",
    "cpu_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "io_bound(0.2)\n",
    "io_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"CPU-bound function took ~{cpu_time:.3f} s\")\n",
    "print(f\"I/O-bound function took ~{io_time:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4152a",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Rough timing experiment\n",
    "\n",
    "In this exercise you will run a tiny timing experiment to get a feel for the numbers.\n",
    "\n",
    "1. Use the provided `cpu_bound` function.\n",
    "2. Call it with different values, for example `50_000`, `200_000`, `500_000`, and measure the time for each.\n",
    "3. For timing, use `time.perf_counter()` like in the example.\n",
    "4. Print the `n` value and the time for each run.\n",
    "\n",
    "Do this in a simple, sequential loop. The goal is to get a feeling for how quickly Python loops slow down as `n` grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94aeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Advanced exercise starter\n",
    "# TODO: run cpu_bound with several n values and measure the time.\n",
    "\n",
    "def cpu_bound(n: int) -> int:\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "ns = [50_000, 200_000, 500_000]\n",
    "\n",
    "# for n in ns:\n",
    "#     start = ...  # time.perf_counter()\n",
    "#     result = cpu_bound(n)\n",
    "#     elapsed = ...\n",
    "#     print(f\"n={n}, elapsed={elapsed:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de941549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=50000, elapsed=0.0027 s\n",
      "n=200000, elapsed=0.0131 s\n",
      "n=500000, elapsed=0.0326 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import time\n",
    "\n",
    "def cpu_bound(n: int) -> int:\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "ns = [50_000, 200_000, 500_000]\n",
    "for n in ns:\n",
    "    start = time.perf_counter()\n",
    "    _ = cpu_bound(n)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"n={n}, elapsed={elapsed:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473ea2f",
   "metadata": {},
   "source": [
    "## Topic 2 - Threads for I/O-bound tasks\n",
    "\n",
    "Because of the GIL, Python threads are usually **not** a good solution for CPU-bound acceleration. But for I/O-bound tasks (waiting for files, network, measurement devices), threads can help hide latency.\n",
    "\n",
    "Typical pattern in lab / measurement setups:\n",
    "\n",
    "- You need to query multiple instruments or devices.\n",
    "- Each device responds in, say, 100 ms.\n",
    "- With sequential code, talking to 5 devices takes roughly 5 x 100 ms.\n",
    "- With threads, you can send requests in parallel and total time can be close to 100 ms.\n",
    "\n",
    "Python module: [`threading`](https://docs.python.org/3/library/threading.html)\n",
    "\n",
    "Note that this does not break the GIL for CPU work, but it is very useful to manage multiple slow I/O operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d575b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor 4: value=1.0400, delay=0.20s\n",
      "Sensor 2: value=1.0798, delay=0.30s\n",
      "Sensor 1: value=0.9523, delay=0.40s\n",
      "Sensor 3: value=0.9374, delay=0.60s\n",
      "Total elapsed (threaded): 0.622 s\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "def measure_sensor(sensor_id: int, delay: float) -> None:\n",
    "    \"\"\"Simulate a measurement: wait 'delay' seconds, then print a value.\"\"\"\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)  # normalized size measurement\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "threads = []\n",
    "start = time.perf_counter()\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    t = threading.Thread(target=measure_sensor, args=(i, d))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Total elapsed (threaded): {elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f009a",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Compare sequential vs threaded measurements\n",
    "\n",
    "Use the `measure_sensor` function idea to compare sequential and threaded execution.\n",
    "\n",
    "1. Re-implement a simple `measure_sensor` that sleeps and prints a message.\n",
    "2. First, call it sequentially in a loop for all delays.\n",
    "3. Then, call it using `threading.Thread` like in the example.\n",
    "4. Measure and print the elapsed time for both cases.\n",
    "\n",
    "You can reuse the list `delays = [0.4, 0.3, 0.6, 0.2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "# TODO: implement sequential and threaded measurement and compare their times.\n",
    "\n",
    "def measure_sensor(sensor_id: int, delay: float) -> None:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "# 1) Sequential run\n",
    "# start = ...\n",
    "# for i, d in enumerate(delays, start=1):\n",
    "#     measure_sensor(i, d)\n",
    "# seq_elapsed = ...\n",
    "# print(f\"Sequential elapsed: {seq_elapsed:.3f} s\")\n",
    "\n",
    "# 2) Threaded run\n",
    "# threads = []\n",
    "# start = ...\n",
    "# for i, d in enumerate(delays, start=1):\n",
    "#     ... create and start threads ...\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "# thr_elapsed = ...\n",
    "# print(f\"Threaded elapsed: {thr_elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de51e38e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor 1: value=1.0886, delay=0.40s\n",
      "Sensor 2: value=0.9940, delay=0.30s\n",
      "Sensor 3: value=0.9065, delay=0.60s\n",
      "Sensor 4: value=1.0301, delay=0.20s\n",
      "Sequential elapsed: 1.505 s\n",
      "Sensor 4: value=1.0565, delay=0.20s\n",
      "Sensor 2: value=0.9084, delay=0.30s\n",
      "Sensor 1: value=0.9360, delay=0.40s\n",
      "Sensor 3: value=1.0763, delay=0.60s\n",
      "Threaded elapsed: 0.608 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "def measure_sensor(sensor_id: int, delay: float) -> None:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "# Sequential\n",
    "start = time.perf_counter()\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    measure_sensor(i, d)\n",
    "seq_elapsed = time.perf_counter() - start\n",
    "print(f\"Sequential elapsed: {seq_elapsed:.3f} s\")\n",
    "\n",
    "# Threaded\n",
    "threads = []\n",
    "start = time.perf_counter()\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    t = threading.Thread(target=measure_sensor, args=(i, d))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "thr_elapsed = time.perf_counter() - start\n",
    "print(f\"Threaded elapsed: {thr_elapsed:.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174bcf1",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Collect results into a shared list\n",
    "\n",
    "Right now, `measure_sensor` just prints values. In real life, we want to **store** results.\n",
    "\n",
    "1. Modify `measure_sensor` so that it appends `(sensor_id, value)` to a shared list.\n",
    "2. Use a `threading.Lock` to protect the shared list.\n",
    "3. After all threads finish, print the collected list of results.\n",
    "\n",
    "This pattern mimics collecting measurement results from multiple devices into a shared in-memory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb41945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "results = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def measure_and_store(sensor_id: int, delay: float) -> None:\n",
    "    \"\"\"TODO: simulate measurement and safely append to results.\"\"\"\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    # with lock:\n",
    "    #     results.append(...)\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "threads = []\n",
    "# TODO: start threads using measure_and_store and join them, then print results.\n",
    "# for i, d in enumerate(delays, start=1):\n",
    "#     t = threading.Thread(target=measure_and_store, args=(i, d))\n",
    "#     threads.append(t)\n",
    "#     t.start()\n",
    "\n",
    "# for t in threads:\n",
    "#     t.join()\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8e31b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected results: [(4, 1.0242552744792301), (2, 0.9700888704122314), (1, 1.0925002232553185), (3, 0.9773592353806989)]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "results = []\n",
    "lock = threading.Lock()\n",
    "\n",
    "def measure_and_store(sensor_id: int, delay: float) -> None:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    with lock:\n",
    "        results.append((sensor_id, value))\n",
    "\n",
    "delays = [0.4, 0.3, 0.6, 0.2]\n",
    "threads = []\n",
    "for i, d in enumerate(delays, start=1):\n",
    "    t = threading.Thread(target=measure_and_store, args=(i, d))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"Collected results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b13168",
   "metadata": {},
   "source": [
    "---\n",
    "# Short break (10:30 - 10:45)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9eddc",
   "metadata": {},
   "source": [
    "## Topic 3 - Processes for CPU-bound workloads\n",
    "\n",
    "Because of the GIL, threads do not speed up CPU-bound pure Python loops. For heavy numerical work in pure Python, we can use **processes** instead of threads.\n",
    "\n",
    "Python module: [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html)\n",
    "\n",
    "A **process** has its own Python interpreter and its own GIL, so multiple processes can truly run in parallel on multiple CPU cores.\n",
    "\n",
    "Typical pattern in scientific computing:\n",
    "\n",
    "- Split a large dataset into chunks (e.g. surface height maps for different samples).\n",
    "- Spawn a pool of worker processes.\n",
    "- Each process computes statistics for one chunk.\n",
    "- Collect results in the main process.\n",
    "\n",
    "Downside: processes have more overhead than threads (especially for sending large arrays between processes), but they allow true CPU-core scaling for pure Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7ca47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using up to 8 CPU cores\n"
     ]
    }
   ],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "def compute_rms(values):\n",
    "    \"\"\"Compute RMS roughness of a list of heights.\"\"\"\n",
    "    s = 0.0\n",
    "    for x in values:\n",
    "        s += x * x\n",
    "    return math.sqrt(s / len(values))\n",
    "\n",
    "def main():\n",
    "    # Create fake measurement data: 4 samples with 100_000 points each\n",
    "    samples = [[random.uniform(-1e-6, 1e-6) for _ in range(100_000)] for _ in range(4)]\n",
    "\n",
    "    print(f\"Using up to {cpu_count()} CPU cores\")\n",
    "    start = time.perf_counter()\n",
    "    with Pool() as pool:\n",
    "        rms_values = pool.map(compute_rms, samples)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Total elapsed (threaded): {elapsed:.3f} s\")\n",
    "    \n",
    "    print(\"RMS roughness per sample:\")\n",
    "    for i, rms in enumerate(rms_values, start=1):\n",
    "        print(f\"  Sample {i}: {rms:.3e} m\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea85b4",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Parallel average diameter per batch\n",
    "\n",
    "Imagine you have several batches of diameter measurements for different parts. Each batch is a list of floats.\n",
    "\n",
    "1. Implement a function `average(values)` that computes the mean of the list.\n",
    "2. Create a list of batches (for example 3-5 lists with random diameters in millimeters).\n",
    "3. Use `multiprocessing.Pool.map` to compute the average diameter for each batch in parallel.\n",
    "4. Print the result for each batch.\n",
    "\n",
    "Keep the batch sizes small enough that the code finishes quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "# TODO: compute average diameter per batch in parallel.\n",
    "\n",
    "def average(values):\n",
    "    # return ...\n",
    "\n",
    "# Create example batches of diameters in mm\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "def main():\n",
    "    # with Pool() as pool:\n",
    "    #     averages = ...\n",
    "    # for i, avg in enumerate(averages, start=1):\n",
    "    #     print(f\"Batch {i}: average diameter = {avg:.3f} mm\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f590d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def average(values):\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    return total / len(values)\n",
    "\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "def main():\n",
    "    with Pool() as pool:\n",
    "        averages = pool.map(average, batches)\n",
    "    \n",
    "    for i, avg in enumerate(averages, start=1):\n",
    "        print(f\"Batch {i}: average diameter = {avg:.3f} mm\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cab95",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Parallel min, max, mean\n",
    "\n",
    "Extend the previous idea to compute **three** statistics per batch: `(min, max, mean)`.\n",
    "\n",
    "1. Write a function `stats(values)` that returns a tuple `(min_value, max_value, mean_value)`.\n",
    "2. Reuse or create batches of diameter or thickness measurements.\n",
    "3. Use a process pool to compute stats for each batch in parallel.\n",
    "4. Print the three statistics for each batch in a readable way.\n",
    "\n",
    "This is close to what you would actually do with per-sample measurement datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aee1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def stats(values):\n",
    "    \"\"\"TODO: return (min, max, mean) for the list.\"\"\"\n",
    "    # m = min(values)\n",
    "    # M = max(values)\n",
    "    # mean = ...\n",
    "    # return m, M, mean\n",
    "    m = min(values)\n",
    "    M = max(values)\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    mean = total / len(values)\n",
    "    return m, M, mean\n",
    "\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "def main():\n",
    "    # TODO: use Pool to compute stats in parallel and print them.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a11fe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def stats(values):\n",
    "    m = min(values)\n",
    "    M = max(values)\n",
    "    total = 0.0\n",
    "    for v in values:\n",
    "        total += v\n",
    "    mean = total / len(values)\n",
    "    return m, M, mean\n",
    "\n",
    "batches = [\n",
    "    [random.uniform(9.95, 10.05) for _ in range(50)],\n",
    "    [random.uniform(4.95, 5.05) for _ in range(80)],\n",
    "    [random.uniform(19.9, 20.1) for _ in range(100)],\n",
    "]\n",
    "\n",
    "def main():\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(stats, batches)\n",
    "    \n",
    "    for i, (m, M, mean) in enumerate(results, start=1):\n",
    "        print(f\"Batch {i}: min={m:.3f}, max={M:.3f}, mean={mean:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f7394-c8be-4c6b-ad8e-df7cda3ad40b",
   "metadata": {},
   "source": [
    "## Topic 4 - AsyncIO for concurrent I/O\n",
    "\n",
    "Python's [`asyncio`](https://docs.python.org/3/library/asyncio.html) module lets you write **concurrent** programs using the `async` / `await` syntax.\n",
    "\n",
    "Unlike `threading` or `multiprocessing`, AsyncIO usually runs **in a single OS thread**. It uses an **event loop** that rapidly switches between many \"tasks\" (coroutines) whenever they are waiting for I/O.\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "- **Coroutine**: a function defined with `async def`. Calling it does **not** run it, it returns a coroutine object (a bit like creating a `Task` in LabVIEW or a job in a scheduler).\n",
    "  ```python\n",
    "  async def measure():\n",
    "      ...\n",
    "  c = measure()  # coroutine object, not a result yet\n",
    "  ```\n",
    "- **Event loop**: a scheduler that runs coroutines. In scripts you usually start it with `asyncio.run(main())`. In Jupyter, the notebook already runs an event loop for you, so you can use `await` directly at the top level.\n",
    "- **`await`**: suspends the current coroutine until the awaited operation is finished, and lets the event loop run other tasks in the meantime.\n",
    "  - You can only use `await` **inside** `async def` functions (or at the top level in special environments like Jupyter).\n",
    "  - This is like saying \"I am waiting for the ADC / network / disk - while I wait, please do something else\".\n",
    "\n",
    "### When does AsyncIO help?\n",
    "\n",
    "AsyncIO is ideal when your program is **I/O bound** and spends most of its time waiting:\n",
    "\n",
    "- Many concurrent HTTP requests (web scraping, microservices).\n",
    "- Talking to many instruments over TCP/serial/USB at once.\n",
    "- File or database operations that frequently wait on the OS.\n",
    "\n",
    "It does **not** make pure Python CPU loops faster. The Global Interpreter Lock (GIL) still limits CPU bound code. For heavy numeric work you usually use:\n",
    "\n",
    "- **NumPy / vectorization** (day 3 topic),\n",
    "- **C / C++ extensions**, or\n",
    "- **multiprocessing** to use multiple cores.\n",
    "\n",
    "### How does control flow work?\n",
    "\n",
    "Think of the event loop as a conductor for an orchestra of coroutines:\n",
    "\n",
    "1. You define `async def` coroutines.\n",
    "2. You create tasks from them (for example with `asyncio.create_task`).\n",
    "3. The event loop picks a task, runs it until it hits an `await` on something that is not yet ready (e.g. `await asyncio.sleep(0.5)` or an async network call).\n",
    "4. At that `await`, the coroutine \"pauses\", the loop switches to another ready task.\n",
    "5. When the awaited I/O operation completes, the loop resumes the paused coroutine right after the `await`.\n",
    "\n",
    "Because tasks yield control at `await`, many of them can **make progress in overlapping time** - even though there is a single underlying OS thread.\n",
    "\n",
    "### Frequently asked questions\n",
    "\n",
    "**Q: Why can I not `await` in a normal function?**  \n",
    "Because the Python grammar only allows `await` inside `async def` (or in special interactive environments). A normal `def` function does not know how to suspend and resume itself. If you want to use `await`, you must either:\n",
    "- make the function `async def`, or\n",
    "- call an async function from the **outside** using `asyncio.run(my_async_main())` in a script.\n",
    "\n",
    "**Q: When is the event loop actually running?**  \n",
    "- In a **script**, when you call `asyncio.run(main())` (which internally creates and runs an event loop).\n",
    "- In **Jupyter**, there is already a loop running; the notebook integrates with it so `await` works at the top level. That is why you can simply do `await main()` in a notebook cell.\n",
    "\n",
    "**Q: How do I actually get real benefits?**  \n",
    "1. Identify I/O operations that can run in parallel (waiting for TCP sockets, serial ports, HTTP, `asyncio.sleep`, etc.).\n",
    "2. Wrap them in `async def` coroutines that `await` the I/O.\n",
    "3. Start many tasks with `asyncio.create_task` or `asyncio.gather`.\n",
    "4. Let the event loop interleave their waiting periods.\n",
    "\n",
    "Below we compare sequential vs async measurements to see the effect in practice.\n",
    "\n",
    "**Further reading:**\n",
    "- Official docs: https://docs.python.org/3/library/asyncio.html\n",
    "- \"Async IO in Python: A Complete Walkthrough\" (Real Python): https://realpython.com/async-io-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a94cecf1-af12-48ab-a24d-66ae4b8a36aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sequential sync measurements ---\n",
      "[sync ] Sensor 1: value=1.0224, delay=0.40s\n",
      "[sync ] Sensor 2: value=1.0988, delay=0.30s\n",
      "[sync ] Sensor 3: value=0.9386, delay=0.60s\n",
      "[sync ] Sensor 4: value=1.0355, delay=0.20s\n",
      "Sync total elapsed: 1.504 s\n",
      "\n",
      "--- Concurrent async measurements ---\n",
      "[async] Sensor 4: value=0.9681, delay=0.20s\n",
      "[async] Sensor 2: value=1.0904, delay=0.30s\n",
      "[async] Sensor 1: value=0.9783, delay=0.40s\n",
      "[async] Sensor 3: value=1.0900, delay=0.60s\n",
      "Async total elapsed: 0.609 s\n",
      "\n",
      "Sync values:  ['1.0224', '1.0988', '0.9386', '1.0355']\n",
      "Async values: ['0.9783', '1.0904', '1.0900', '0.9681']\n",
      "Max individual delay: 0.6 s\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "# Synchronous version: measure each sensor one after the other\n",
    "\n",
    "def measure_sensor_sync(sensor_id: int, delay: float) -> float:\n",
    "    \"\"\"Simulate a blocking sensor measurement using time.sleep.\"\"\"\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"[sync ] Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "# Async version: non-blocking wait with asyncio.sleep\n",
    "\n",
    "async def measure_sensor_async(sensor_id: int, delay: float) -> float:\n",
    "    \"\"\"Async measurement: uses await asyncio.sleep instead of time.sleep.\"\"\"\n",
    "    await asyncio.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"[async] Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "async def main_compare_sync_vs_async() -> None:\n",
    "    delays = [0.4, 0.3, 0.6, 0.2]\n",
    "\n",
    "    print(\"\\n--- Sequential sync measurements ---\")\n",
    "    start_sync = time.perf_counter()\n",
    "    sync_values = [measure_sensor_sync(i, d) for i, d in enumerate(delays, start=1)]\n",
    "    elapsed_sync = time.perf_counter() - start_sync\n",
    "    print(f\"Sync total elapsed: {elapsed_sync:.3f} s\")\n",
    "\n",
    "    print(\"\\n--- Concurrent async measurements ---\")\n",
    "    start_async = time.perf_counter()\n",
    "    # Create tasks for all sensors\n",
    "    tasks = [asyncio.create_task(measure_sensor_async(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    # Wait for all tasks to finish\n",
    "    async_values = await asyncio.gather(*tasks)\n",
    "    elapsed_async = time.perf_counter() - start_async\n",
    "    print(f\"Async total elapsed: {elapsed_async:.3f} s\")\n",
    "\n",
    "    print(\"\\nSync values: \", [f\"{v:.4f}\" for v in sync_values])\n",
    "    print(\"Async values:\", [f\"{v:.4f}\" for v in async_values])\n",
    "    print(\"Max individual delay:\", max(delays), \"s\")\n",
    "\n",
    "# In a Jupyter notebook you can run the async main with top-level await:\n",
    "await main_compare_sync_vs_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f6fd8-9fcf-40e0-9ed2-8eb9fd3d5672",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Measure async speedup factor\n",
    "\n",
    "Using the example above as a starting point:\n",
    "\n",
    "1. Copy the idea of `measure_sensor_async` and `asyncio.gather` into a new async function, for example `async_measure_many_sensors()`.\n",
    "2. Use a list of delays like `[0.1, 0.5, 0.2, 0.8, 0.3]`.\n",
    "3. Measure\n",
    "   - how long it would take to run all measurements **sequentially** (with `time.sleep`), and\n",
    "   - how long it takes to run them **concurrently** with AsyncIO.\n",
    "4. Print the **speedup factor** as `sync_time / async_time`.\n",
    "\n",
    "You already saw everything you need:\n",
    "- `time.perf_counter()` for timing,\n",
    "- `asyncio.create_task` + `asyncio.gather` for running tasks concurrently,\n",
    "- `await` inside `async def`.\n",
    "\n",
    "Think about why the async time is close to the **maximum** delay instead of the **sum** of delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b68ca-29d9-4d6f-b6a8-d677e1385451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "# You can reuse or adapt these building blocks\n",
    "\n",
    "def measure_sensor_sync(sensor_id: int, delay: float) -> float:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"[sync ] Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "async def measure_sensor_async(sensor_id: int, delay: float) -> float:\n",
    "    await asyncio.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    print(f\"[async] Sensor {sensor_id}: value={value:.4f}, delay={delay:.2f}s\")\n",
    "    return value\n",
    "\n",
    "async def async_measure_many_sensors() -> None:\n",
    "    delays = [0.1, 0.5, 0.2, 0.8, 0.3]\n",
    "\n",
    "    # TODO:\n",
    "    # 1) Measure sequential sync time using measure_sensor_sync.\n",
    "    # 2) Measure async time by creating tasks and awaiting asyncio.gather.\n",
    "    # 3) Print both times and the speedup factor (sync_time / async_time).\n",
    "    pass\n",
    "\n",
    "# TODO: run async_measure_many_sensors from this cell using await.\n",
    "# Example:\n",
    "# await async_measure_many_sensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e704fc5c-aa0c-4d7f-aa70-bf16931b1a11",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync elapsed:  1.904 s\n",
      "Async elapsed: 0.807 s\n",
      "Speedup:       2.36x\n",
      "Sync values:  ['1.0394', '1.0554', '0.9323', '0.9256', '0.9803']\n",
      "Async values: ['0.9262', '1.0007', '1.0895', '0.9597', '0.9325']\n"
     ]
    }
   ],
   "source": [
    "# Example solution for the easy async speedup exercise\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "from random import uniform\n",
    "\n",
    "\n",
    "def measure_sensor_sync(sensor_id: int, delay: float) -> float:\n",
    "    time.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    return value\n",
    "\n",
    "async def measure_sensor_async(sensor_id: int, delay: float) -> float:\n",
    "    await asyncio.sleep(delay)\n",
    "    value = uniform(0.9, 1.1)\n",
    "    return value\n",
    "\n",
    "async def async_measure_many_sensors() -> None:\n",
    "    delays = [0.1, 0.5, 0.2, 0.8, 0.3]\n",
    "\n",
    "    # Sequential measurements\n",
    "    start_sync = time.perf_counter()\n",
    "    sync_values = [measure_sensor_sync(i, d) for i, d in enumerate(delays, start=1)]\n",
    "    elapsed_sync = time.perf_counter() - start_sync\n",
    "\n",
    "    # Async concurrent measurements\n",
    "    start_async = time.perf_counter()\n",
    "    tasks = [asyncio.create_task(measure_sensor_async(i, d))\n",
    "             for i, d in enumerate(delays, start=1)]\n",
    "    async_values = await asyncio.gather(*tasks)\n",
    "    elapsed_async = time.perf_counter() - start_async\n",
    "\n",
    "    speedup = elapsed_sync / elapsed_async if elapsed_async > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"Sync elapsed:  {elapsed_sync:.3f} s\")\n",
    "    print(f\"Async elapsed: {elapsed_async:.3f} s\")\n",
    "    print(f\"Speedup:       {speedup:.2f}x\")\n",
    "    print(\"Sync values: \", [f\"{v:.4f}\" for v in sync_values])\n",
    "    print(\"Async values:\", [f\"{v:.4f}\" for v in async_values])\n",
    "\n",
    "await async_measure_many_sensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c619540",
   "metadata": {},
   "source": [
    "---\n",
    "# Lunch break (12:00 - 13:00)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e944fc6",
   "metadata": {},
   "source": [
    "## Topic 5 - NumPy recap and basic vectorization\n",
    "\n",
    "[NumPy](https://numpy.org/) is the standard array library for numerical computing in Python.\n",
    "\n",
    "Key ideas:\n",
    "\n",
    "- A `numpy.ndarray` is an efficient, typed, homogeneous n-dimensional array.\n",
    "- Many operations are implemented in optimized C code.\n",
    "- Operations like `a + b`, `a * b` on arrays are **vectorized**: they run in fast loops in C rather than Python.\n",
    "\n",
    "In measurement and physics workflows, NumPy is ideal for:\n",
    "\n",
    "- Handling long arrays of measured values (heights, diameters, thicknesses, voltages).\n",
    "- Computing statistics and transformations (offset correction, unit conversion, normalization).\n",
    "- Applying elementwise functions (e.g. calibration curves, non-linear corrections).\n",
    "\n",
    "Make sure you have NumPy installed. In this course environment it should already be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw thickness (um): [100.2  99.8 100.5 100.1  99.9 100.3 100.   99.7 100.4 100.1]\n",
      "Shape: (10,) dtype: float64\n",
      "Thickness (mm): [0.1002 0.0998 0.1005 0.1001 0.0999 0.1003 0.1    0.0997 0.1004 0.1001]\n",
      "Mean (um): 100.1\n",
      "Std (um): 0.24494897427831783\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Thickness measurements in micrometers for 10 samples\n",
    "thickness_um = np.array([100.2, 99.8, 100.5, 100.1, 99.9, 100.3, 100.0, 99.7, 100.4, 100.1])\n",
    "print(\"Raw thickness (um):\", thickness_um)\n",
    "print(\"Shape:\", thickness_um.shape, \"dtype:\", thickness_um.dtype)\n",
    "\n",
    "# Convert to millimeters\n",
    "thickness_mm = thickness_um / 1000.0\n",
    "print(\"Thickness (mm):\", thickness_mm)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Mean (um):\", thickness_um.mean())\n",
    "print(\"Std (um):\", thickness_um.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449ce64",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Diameter conversion and simple statistics\n",
    "\n",
    "1. Create a NumPy array of diameters in millimeters for at least 8 parts.\n",
    "2. Convert them to micrometers (multiply by 1000).\n",
    "3. Compute and print the mean and standard deviation in micrometers.\n",
    "\n",
    "Use the same patterns as in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: diameter conversion and basic statistics with NumPy.\n",
    "\n",
    "# diam_mm = np.array([...])\n",
    "# diam_um = ...\n",
    "# mean_um = ...\n",
    "# std_um = ...\n",
    "# print(\"Diameters (um):\", diam_um)\n",
    "# print(\"Mean (um):\", mean_um)\n",
    "# print(\"Std (um):\", std_um)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64f6237",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diameters (um): [10010.  9990. 10020. 10000. 10030.  9980. 10010.  9970.]\n",
      "Mean (um): 10001.25\n",
      "Std (um): 18.99835519196333\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "diam_mm = np.array([10.01, 9.99, 10.02, 10.00, 10.03, 9.98, 10.01, 9.97])\n",
    "diam_um = diam_mm * 1000.0\n",
    "mean_um = diam_um.mean()\n",
    "std_um = diam_um.std()\n",
    "print(\"Diameters (um):\", diam_um)\n",
    "print(\"Mean (um):\", mean_um)\n",
    "print(\"Std (um):\", std_um)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd42de",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Offset correction and normalized values\n",
    "\n",
    "Imagine your thickness sensor has a calibration offset error of +0.3 micrometers.\n",
    "\n",
    "1. Create an array of measured thicknesses in micrometers.\n",
    "2. Subtract the offset from all values to get corrected thicknesses.\n",
    "3. Compute the mean and standard deviation of the corrected values.\n",
    "4. Compute a normalized array where you subtract the mean and divide by the standard deviation.\n",
    "\n",
    "This pattern (offset correction + normalization) is common in data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe711695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: offset correction and normalization.\n",
    "# thickness_um = np.array([...])\n",
    "# offset = 0.3\n",
    "# corrected = ...  # subtract offset\n",
    "# mean_corr = ...\n",
    "# std_corr = ...\n",
    "# normalized = ...  # (corrected - mean_corr) / std_corr\n",
    "# print(\"Corrected thickness (um):\", corrected)\n",
    "# print(\"Normalized values:\", normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6355e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected thickness (um): [ 99.9  99.7  99.6 100.1  99.8  99.5]\n",
      "Normalized values: [ 0.6761234  -0.3380617  -0.84515425  1.69030851  0.16903085 -1.35224681]\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "thickness_um = np.array([100.2, 100.0, 99.9, 100.4, 100.1, 99.8])\n",
    "offset = 0.3\n",
    "corrected = thickness_um - offset\n",
    "mean_corr = corrected.mean()\n",
    "std_corr = corrected.std()\n",
    "normalized = (corrected - mean_corr) / std_corr\n",
    "print(\"Corrected thickness (um):\", corrected)\n",
    "print(\"Normalized values:\", normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e7649",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Topic 6 - NumPy vectorized computations in practice\n",
    "\n",
    "Now we go deeper into NumPy vectorization. We will use:\n",
    "\n",
    "- Elementwise operations on arrays\n",
    "- Boolean masks and filtering\n",
    "- Aggregations along axes (2D arrays)\n",
    "- Simple physics-themed computations\n",
    "\n",
    "NumPy allows you to write **array expressions** instead of Python `for` loops. These expressions are executed in optimized C loops under the hood.\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- NumPy user guide: https://numpy.org/doc/stable/user/index.html\n",
    "- NumPy quickstart: https://numpy.org/doc/stable/user/quickstart.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759bf60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diameters: [10.01  9.97 10.05 10.02  9.94 10.    9.99]\n",
      "Deviation: [ 0.01 -0.03  0.05  0.02 -0.06  0.   -0.01]\n",
      "In spec mask: [ True  True False  True False  True  True]\n",
      "In spec diameters: [10.01  9.97 10.02 10.    9.99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: filter diameters by tolerance\n",
    "diam_mm = np.array([10.01, 9.97, 10.05, 10.02, 9.94, 10.00, 9.99])\n",
    "target = 10.00\n",
    "tolerance = 0.03\n",
    "\n",
    "deviation = diam_mm - target\n",
    "mask_in_spec = np.abs(deviation) <= tolerance\n",
    "print(\"Diameters:\", diam_mm)\n",
    "print(\"Deviation:\", deviation)\n",
    "print(\"In spec mask:\", mask_in_spec)\n",
    "print(\"In spec diameters:\", diam_mm[mask_in_spec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28be655",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Filter out-of-spec samples\n",
    "\n",
    "1. Create a NumPy array of length measurements in millimeters.\n",
    "2. Define a target and tolerance.\n",
    "3. Build a mask of samples that are **out of spec** (absolute deviation larger than tolerance).\n",
    "4. Print the array of out-of-spec values and their count.\n",
    "\n",
    "Use boolean masks like in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: filter out-of-spec length values using a boolean mask.\n",
    "\n",
    "# lengths_mm = np.array([...])\n",
    "# target = ...\n",
    "# tolerance = ...\n",
    "# deviation = ...\n",
    "# mask_out = ...  # abs(deviation) > tolerance\n",
    "# print(\"Out-of-spec values:\", lengths_mm[mask_out])\n",
    "# print(\"Count:\", mask_out.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1d642e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-spec values: [49.95 50.04 49.92]\n",
      "Count: 3\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lengths_mm = np.array([49.98, 50.02, 49.95, 50.04, 50.01, 49.92])\n",
    "target = 50.00\n",
    "tolerance = 0.03\n",
    "deviation = lengths_mm - target\n",
    "mask_out = np.abs(deviation) > tolerance\n",
    "print(\"Out-of-spec values:\", lengths_mm[mask_out])\n",
    "print(\"Count:\", int(mask_out.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ecefb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üí™ Exercise (advanced): Repeated measurements per part (2D arrays)\n",
    "\n",
    "Suppose you measure the same part multiple times to estimate uncertainty.\n",
    "\n",
    "1. Create a 2D NumPy array `data` of shape `(n_parts, n_repeats)` containing diameters in mm.\n",
    "2. Compute the mean diameter **per part** (axis 1).\n",
    "3. Compute the standard deviation per part (axis 1).\n",
    "4. Compute the overall mean of all measurements.\n",
    "5. Print the per-part mean and standard deviation.\n",
    "\n",
    "You should use `data.mean(axis=1)` and `data.std(axis=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: repeated measurement statistics with 2D arrays.\n",
    "\n",
    "# Example: 4 parts, 5 repeated measurements each\n",
    "# data = np.array([\n",
    "#     [...],\n",
    "#     [...],\n",
    "# ])\n",
    "\n",
    "# mean_per_part = ...  # data.mean(axis=1)\n",
    "# std_per_part = ...   # data.std(axis=1)\n",
    "# overall_mean = ...   # data.mean()\n",
    "\n",
    "# print(\"Mean per part:\", mean_per_part)\n",
    "# print(\"Std per part:\", std_per_part)\n",
    "# print(\"Overall mean:\", overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e901fac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean per part: [10.  5. 20. 30.]\n",
      "Std per part: [0.01414214 0.01414214 0.01414214 0.01414214]\n",
      "Overall mean: 16.25\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([\n",
    "    [10.01, 9.99, 10.00, 10.02, 9.98],\n",
    "    [4.99, 5.01, 5.00, 5.02, 4.98],\n",
    "    [19.98, 20.00, 20.01, 19.99, 20.02],\n",
    "    [29.99, 30.01, 30.00, 30.02, 29.98],\n",
    "])\n",
    "\n",
    "mean_per_part = data.mean(axis=1)\n",
    "std_per_part = data.std(axis=1)\n",
    "overall_mean = data.mean()\n",
    "\n",
    "print(\"Mean per part:\", mean_per_part)\n",
    "print(\"Std per part:\", std_per_part)\n",
    "print(\"Overall mean:\", overall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ae6db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Vectorized physics-style computation example\n",
    "\n",
    "As a small example, imagine you measured heights `h` at given positions `x` along a line, and you want to approximate the area under the curve using the trapezoidal rule.\n",
    "\n",
    "The trapezoidal rule for arrays `x` and `h` can be written as:\n",
    "\n",
    "`area ‚âà sum( (h[i] + h[i+1]) / 2 * (x[i+1] - x[i]) )`\n",
    "\n",
    "We can implement this with pure NumPy, using slicing, without explicit Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd2100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate area (um*mm): 100.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate positions in mm and heights in micrometers\n",
    "x = np.linspace(0.0, 10.0, 1001)  # 0..10 mm, 1001 points\n",
    "h_um = 2.0 * np.sin(2 * np.pi * x / 10.0) + 10.0  # some periodic height pattern in um\n",
    "\n",
    "# Trapezoidal rule using vectorized slices\n",
    "dx = x[1:] - x[:-1]\n",
    "h_avg = (h_um[1:] + h_um[:-1]) / 2.0\n",
    "area_um_mm = np.sum(h_avg * dx)  # units: um * mm\n",
    "\n",
    "print(f\"Approximate area (um*mm): {area_um_mm:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87a572",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üí™ Exercise (advanced - optional): Chain of vectorized operations\n",
    "\n",
    "Combine several vectorized operations into a small pipeline:\n",
    "\n",
    "1. Simulate an array of thicknesses in micrometers with `np.random.normal`.\n",
    "2. Apply an offset correction.\n",
    "3. Clip the values to a realistic range using `np.clip`.\n",
    "4. Convert to millimeters.\n",
    "5. Compute and print the mean and standard deviation in both units.\n",
    "\n",
    "Do not use explicit Python loops. Use NumPy array operations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31633097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: implement the vectorized processing pipeline.\n",
    "\n",
    "# n = 1000\n",
    "# thickness_um = np.random.normal(loc=100.0, scale=0.5, size=n)\n",
    "# offset = 0.2\n",
    "# corrected = ...\n",
    "# clipped = ...  # np.clip\n",
    "# thickness_mm = ...\n",
    "# print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4c592d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (um): 99.80521127498606\n",
      "Std (um): 0.5115783054404365\n",
      "Mean (mm): 0.09980521127498607\n",
      "Std (mm): 0.0005115783054404363\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "thickness_um = np.random.normal(loc=100.0, scale=0.5, size=n)\n",
    "offset = 0.2\n",
    "corrected = thickness_um - offset\n",
    "clipped = np.clip(corrected, 98.0, 102.0)\n",
    "thickness_mm = clipped / 1000.0\n",
    "\n",
    "print(\"Mean (um):\", clipped.mean())\n",
    "print(\"Std (um):\", clipped.std())\n",
    "print(\"Mean (mm):\", thickness_mm.mean())\n",
    "print(\"Std (mm):\", thickness_mm.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8572f",
   "metadata": {},
   "source": [
    "---\n",
    "# Short break (14:45 - 15:00)\n",
    "\n",
    "Final stretch: Numba, GPUs, Python 3.13, and a complex example.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9c68c",
   "metadata": {},
   "source": [
    "## Topic 7 - Numba JIT compilation\n",
    "\n",
    "[Numba](https://numba.pydata.org/) is a Just-In-Time (JIT) compiler for Python functions that operate mainly on NumPy arrays and numbers.\n",
    "\n",
    "Basic usage:\n",
    "\n",
    "```python\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def f(x):\n",
    "    # numerical code\n",
    "    ...\n",
    "```\n",
    "\n",
    "When you first call `f`, Numba compiles it to machine code (using LLVM). Subsequent calls run at near-C speed.\n",
    "\n",
    "### Important: how Numba and NumPy interact\n",
    "\n",
    "- NumPy operations like `a + b` or `a.mean()` are already implemented in C.\n",
    "- However, **Python loops around those operations** still run in the Python interpreter.\n",
    "- Numba helps most when you have **custom loops and logic** that cannot be expressed as simple NumPy expressions.\n",
    "\n",
    "**Answering the question:** \"If Numba works with NumPy + Python code, and NumPy is already implemented in C, how does Numba JIT help?\"\n",
    "\n",
    "- NumPy is fast for each individual operation, but if you write Python like:\n",
    "  - `for i in range(n): result[i] = complex_expression(a[i], b[i])`\n",
    "  - this loop runs in Python and pays Python overhead per iteration.\n",
    "- Numba compiles the **whole loop** (and the operations inside it) into one optimized machine code function.\n",
    "- This removes Python overhead and can fuse several operations into one pass.\n",
    "\n",
    "In practice, combine them:\n",
    "\n",
    "- Use NumPy vectorization where it is natural.\n",
    "- Use Numba for custom numeric kernels that are hard to write as a single NumPy expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f86e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python RMS: 0.999263, time=0.0114 s\n",
      "Numba RMS first call: 0.999263, time=1.6283 s (includes compile)\n",
      "Numba RMS second call: 0.999263, time=0.0001 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba is not installed in this environment. The examples will fall back to pure Python.\")\n",
    "\n",
    "def rms_python(arr: np.ndarray) -> float:\n",
    "    s = 0.0\n",
    "    n = arr.size\n",
    "    for i in range(n):\n",
    "        x = float(arr[i])\n",
    "        s += x * x\n",
    "    return math.sqrt(s / n)\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def rms_numba(arr):\n",
    "        s = 0.0\n",
    "        n = arr.size\n",
    "        for i in range(n):\n",
    "            x = arr[i]\n",
    "            s += x * x\n",
    "        return math.sqrt(s / n)\n",
    "else:\n",
    "    rms_numba = None\n",
    "\n",
    "# Test on a large array\n",
    "arr = np.random.normal(loc=0.0, scale=1.0, size=1_000_00)\n",
    "\n",
    "# Python version\n",
    "start = time.perf_counter()\n",
    "r1 = rms_python(arr)\n",
    "t_python = time.perf_counter() - start\n",
    "print(f\"Python RMS: {r1:.6f}, time={t_python:.4f} s\")\n",
    "\n",
    "if rms_numba is not None:\n",
    "    # First call includes compilation time\n",
    "    start = time.perf_counter()\n",
    "    r2 = rms_numba(arr)\n",
    "    t_first = time.perf_counter() - start\n",
    "    # Second call is fast\n",
    "    start = time.perf_counter()\n",
    "    r3 = rms_numba(arr)\n",
    "    t_numba = time.perf_counter() - start\n",
    "    print(f\"Numba RMS first call: {r2:.6f}, time={t_first:.4f} s (includes compile)\")\n",
    "    print(f\"Numba RMS second call: {r3:.6f}, time={t_numba:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3c3f3",
   "metadata": {},
   "source": [
    "### ‚úè Exercise (easy): Numba-accelerated difference of squares\n",
    "\n",
    "1. Implement a function `diff_squares_python(a, b)` that for each element computes `a[i]**2 - b[i]**2`.\n",
    "2. Time it on large NumPy arrays.\n",
    "3. If Numba is available, implement `diff_squares_numba` with `@njit`.\n",
    "4. Compare the timings.\n",
    "\n",
    "Make sure you do not create new Python lists inside the function. Work directly with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb29f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - you can still implement the pure Python version.\")\n",
    "\n",
    "def diff_squares_python(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    n = a.size\n",
    "    out = np.empty_like(a)\n",
    "    # for i ...\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def diff_squares_numba(a, b):\n",
    "        n = a.size\n",
    "        out = np.empty_like(a)\n",
    "        for i # ... (same logic as above)\n",
    "        return out\n",
    "\n",
    "# a = np.random.normal(size=200_000)\n",
    "# b = np.random.normal(size=200_000)\n",
    "# start = time.perf_counter()\n",
    "# out_py = diff_squares_python(a, b)\n",
    "# t_py = time.perf_counter() - start\n",
    "# print(f\"Python time: {t_py:.4f} s\")\n",
    "\n",
    "# if njit is not None:\n",
    "#     start = time.perf_counter()\n",
    "#     out_nb1 = diff_squares_numba(a, b)\n",
    "#     t_nb1 = time.perf_counter() - start\n",
    "#     start = time.perf_counter()\n",
    "#     out_nb2 = diff_squares_numba(a, b)\n",
    "#     t_nb2 = time.perf_counter() - start\n",
    "#     print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7881d0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python time: 0.1173 s\n",
      "Numba first call: 0.5256 s, second call: 0.0005 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - only Python version will run.\")\n",
    "\n",
    "def diff_squares_python(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    n = a.size\n",
    "    out = np.empty_like(a)\n",
    "    for i in range(n):\n",
    "        out[i] = a[i] * a[i] - b[i] * b[i]\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def diff_squares_numba(a, b):\n",
    "        n = a.size\n",
    "        out = np.empty_like(a)\n",
    "        for i in range(n):\n",
    "            out[i] = a[i] * a[i] - b[i] * b[i]\n",
    "        return out\n",
    "\n",
    "a = np.random.normal(size=200_000)\n",
    "b = np.random.normal(size=200_000)\n",
    "\n",
    "start = time.perf_counter()\n",
    "out_py = diff_squares_python(a, b)\n",
    "t_py = time.perf_counter() - start\n",
    "print(f\"Python time: {t_py:.4f} s\")\n",
    "\n",
    "if njit is not None:\n",
    "    start = time.perf_counter()\n",
    "    out_nb1 = diff_squares_numba(a, b)\n",
    "    t_nb1 = time.perf_counter() - start\n",
    "    start = time.perf_counter()\n",
    "    out_nb2 = diff_squares_numba(a, b)\n",
    "    t_nb2 = time.perf_counter() - start\n",
    "    print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de043d52",
   "metadata": {},
   "source": [
    "### üí™ Exercise (advanced): Custom statistic with Numba\n",
    "\n",
    "Define a custom function that is harder to express with pure NumPy:\n",
    "\n",
    "1. Implement `moving_rms_python(arr, window)` that computes RMS roughness over a sliding window.\n",
    "   - For each position `i`, compute RMS of `arr[i : i+window]`.\n",
    "2. If Numba is available, implement `moving_rms_numba` using `@njit`.\n",
    "3. Compare performance on a large array.\n",
    "\n",
    "This is a common pattern when analyzing profiles from surface measurement devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - you can still implement the Python version.\")\n",
    "\n",
    "def moving_rms_python(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    \"\"\"TODO: pure Python moving RMS.\"\"\"\n",
    "    n = arr.size\n",
    "    out = np.empty(n - window + 1, dtype=float)\n",
    "    for i in range(n - window + 1):\n",
    "        sum_squares = 0.0\n",
    "\n",
    "        # For each position i, compute RMS of arr[i : i+window].\n",
    "        for j in range(window):\n",
    "            # ...\n",
    "        out[i] = math.sqrt(sum_squares / window)\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def moving_rms_numba(arr, window):\n",
    "        n = arr.size\n",
    "        out = np.empty(n - window + 1, dtype=float)\n",
    "        for i in range(n - window + 1):\n",
    "            sum_squares = 0.0\n",
    "    \n",
    "            # For each position i, compute RMS of arr[i : i+window].\n",
    "            for j in range(window):\n",
    "                # ... # Same logic as above\n",
    "            out[i] = math.sqrt(sum_squares / window)\n",
    "        return out\n",
    "\n",
    "# arr = np.random.normal(size=200_000)\n",
    "# window = 50\n",
    "# start = time.perf_counter()\n",
    "# r_py = moving_rms_python(arr, window)\n",
    "# t_py = time.perf_counter() - start\n",
    "# print(f\"Python moving RMS time: {t_py:.4f} s\")\n",
    "\n",
    "# if njit is not None:\n",
    "#     start = time.perf_counter()\n",
    "#     r_nb1 = moving_rms_numba(arr, window)\n",
    "#     t_nb1 = time.perf_counter() - start\n",
    "#     start = time.perf_counter()\n",
    "#     r_nb2 = moving_rms_numba(arr, window)\n",
    "#     t_nb2 = time.perf_counter() - start\n",
    "#     print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f197590",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python moving RMS time: 1.4391 s\n",
      "Numba first call: 0.2612 s, second call: 0.0080 s\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "    print(\"Numba not available - only Python version will run.\")\n",
    "\n",
    "def moving_rms_python(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    n = arr.size\n",
    "    out = np.empty(n - window + 1, dtype=float)\n",
    "    for i in range(n - window + 1):\n",
    "        s = 0.0\n",
    "        for j in range(window):\n",
    "            x = float(arr[i + j])\n",
    "            s += x * x\n",
    "        out[i] = math.sqrt(s / window)\n",
    "    return out\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def moving_rms_numba(arr, window):\n",
    "        n = arr.size\n",
    "        out = np.empty(n - window + 1, dtype=np.float64)\n",
    "        for i in range(n - window + 1):\n",
    "            s = 0.0\n",
    "            for j in range(window):\n",
    "                x = arr[i + j]\n",
    "                s += x * x\n",
    "            out[i] = math.sqrt(s / window)\n",
    "        return out\n",
    "\n",
    "arr = np.random.normal(size=200_000)\n",
    "window = 50\n",
    "\n",
    "start = time.perf_counter()\n",
    "r_py = moving_rms_python(arr, window)\n",
    "t_py = time.perf_counter() - start\n",
    "print(f\"Python moving RMS time: {t_py:.4f} s\")\n",
    "\n",
    "if njit is not None:\n",
    "    start = time.perf_counter()\n",
    "    r_nb1 = moving_rms_numba(arr, window)\n",
    "    t_nb1 = time.perf_counter() - start\n",
    "    start = time.perf_counter()\n",
    "    r_nb2 = moving_rms_numba(arr, window)\n",
    "    t_nb2 = time.perf_counter() - start\n",
    "    print(f\"Numba first call: {t_nb1:.4f} s, second call: {t_nb2:.4f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51ab2b",
   "metadata": {},
   "source": [
    "## Topic 8 - GPU acceleration overview: cuDF and CuPy\n",
    "\n",
    "For very large datasets and heavy numerical work, GPUs can be useful.\n",
    "\n",
    "Two popular libraries in the Python ecosystem:\n",
    "\n",
    "- [CuPy](https://cupy.dev/):\n",
    "  - NumPy-like interface for arrays stored on a CUDA GPU.\n",
    "  - Many functions mirror the NumPy API (`cupy.array`, `cupy.mean`, etc.).\n",
    "- [cuDF](https://docs.rapids.ai/api/cudf/stable/):\n",
    "  - Part of the RAPIDS ecosystem: https://rapids.ai/\n",
    "  - Pandas-like DataFrame library running on the GPU.\n",
    "\n",
    "In many cases, the workflow is:\n",
    "\n",
    "- Move large arrays or tables to the GPU once.\n",
    "- Perform many operations there.\n",
    "- Move reduced results (e.g. aggregates) back to the CPU.\n",
    "\n",
    "In this course environment, GPUs might not be available, so the examples below are illustrative only. Do not worry if they raise `ImportError` - that is expected on a CPU-only machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba512f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy is not installed. This example is for illustration only.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "N = 100_000_000  # size of the array\n",
    "\n",
    "print(f\"Array size: {N:,}\")\n",
    "\n",
    "# ---------------- CPU: NumPy ----------------\n",
    "t0 = time.perf_counter()\n",
    "a_cpu = np.random.normal(size=N)\n",
    "mean_cpu = a_cpu.mean()\n",
    "std_cpu = a_cpu.std()\n",
    "t1 = time.perf_counter()\n",
    "cpu_time = t1 - t0\n",
    "\n",
    "print(f\"[NumPy / CPU]  mean={mean_cpu:.5f}, std={std_cpu:.5f}, time={cpu_time:.4f} s\")\n",
    "\n",
    "# ---------------- GPU: CuPy (if available) ----------------\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(\"CuPy version:\", cp.__version__)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    a_gpu = cp.random.normal(size=N)\n",
    "    mean_gpu = a_gpu.mean()\n",
    "    std_gpu = a_gpu.std()\n",
    "    t1 = time.perf_counter()\n",
    "    gpu_time = t1 - t0\n",
    "\n",
    "    print(f\"[CuPy / GPU]  mean={float(mean_gpu):.5f}, std={float(std_gpu):.5f}, time={gpu_time:.4f} s\")\n",
    "    print(f\"Speedup (CPU time / GPU time): {cpu_time / gpu_time:.2f}x\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"CuPy is not installed. This example is for illustration only.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf814b48",
   "metadata": {},
   "source": [
    "## Topic 9 - Python 3.13 and the future: experimental JIT and optional GIL\n",
    "\n",
    "Recent and upcoming CPython releases are adding major performance features:\n",
    "\n",
    "- **Python 3.13** (released in 2024) includes:\n",
    "  - An experimental **free-threaded build** (optional no-GIL mode). See PEP 703.\n",
    "  - An experimental **JIT compiler** (PEP 744) that can speed up some workloads.\n",
    "  - These features are **off by default** and require special builds / flags.\n",
    "- Future versions (3.14 and beyond) are expected to improve JIT performance and evolve the no-GIL story.\n",
    "\n",
    "What this means for you in the medium term:\n",
    "\n",
    "- Well-written numeric Python might become faster without you changing code.\n",
    "- True multi-threaded CPU-bound Python code may become possible without having to use `multiprocessing`.\n",
    "- Libraries like Numba, cuDF, CuPy, and others will likely evolve to take advantage of new capabilities.\n",
    "\n",
    "Official resources:\n",
    "\n",
    "- What's new in Python 3.13: https://docs.python.org/3/whatsnew/3.13.html\n",
    "- PEP 703 (optional GIL): https://peps.python.org/pep-0703/\n",
    "- PEP 744 (JIT): https://peps.python.org/pep-0744/\n",
    "\n",
    "For now, you should still learn threads, processes, AsyncIO, NumPy, and Numba - these skills remain valuable regardless of how the interpreter evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7fb711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python version: 3.13.7 (main, Sep  2 2025, 14:16:00) [MSC v.1944 64 bit (AMD64)]\n",
      "Executable: C:\\Users\\gregk\\Desktop\\winpython\\WPy64-31700\\python\\python.exe\n",
      "Note: in standard CPython 3.13+, JIT and no-GIL builds are optional and may need explicit enabling.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Running Python version:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"Note: in standard CPython 3.13+, JIT and no-GIL builds are optional and may need explicit enabling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5823c",
   "metadata": {},
   "source": [
    "## Topic 10 - Complex example: parallel processing of measurement datasets\n",
    "\n",
    "In this final example we combine multiple ideas from today:\n",
    "\n",
    "- NumPy arrays and vectorized computations\n",
    "- Numba JIT for a custom numeric kernel (optional)\n",
    "- `multiprocessing` to process multiple independent datasets in parallel\n",
    "\n",
    "### Scenario\n",
    "\n",
    "You have several measurement files from a surface profiler. Each file contains a 1D height profile (in micrometers). For each profile, you want to:\n",
    "\n",
    "1. Load the data (in this notebook we will just simulate it).\n",
    "2. Apply an offset correction (subtract mean).\n",
    "3. Compute RMS roughness and peak-to-valley height (max - min).\n",
    "4. Return a small summary dictionary.\n",
    "\n",
    "Then, for many profiles (e.g. 8 or 16), you want to process them in parallel using multiple CPU cores.\n",
    "\n",
    "We will build:\n",
    "\n",
    "- A pure NumPy summary function.\n",
    "- Optionally a Numba-accelerated variant.\n",
    "- A small wrapper that can be used with `multiprocessing.Pool.map`.\n",
    "\n",
    "Your task is to fill in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "\n",
    "def simulate_profile(n_points: int = 50_000) -> np.ndarray:\n",
    "    \"\"\"Simulate a 1D surface profile in micrometers.\"\"\"\n",
    "    x = np.linspace(0.0, 10.0, n_points)\n",
    "    base = 5.0 * np.sin(2 * np.pi * x / 5.0)\n",
    "    noise = np.random.normal(loc=0.0, scale=0.5, size=n_points)\n",
    "    return base + noise\n",
    "\n",
    "def summarize_profile_numpy(profile: np.ndarray) -> dict:\n",
    "    \"\"\"TODO: center profile and compute RMS and peak-to-valley using NumPy only.\"\"\"\n",
    "    # mean = ...\n",
    "    # centered = ...\n",
    "    # rms = ...\n",
    "    # ptv = ...\n",
    "    # return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": float(ptv)}\n",
    "    mean = profile.mean()\n",
    "    centered = profile - mean\n",
    "    rms = math.sqrt((centered * centered).mean())\n",
    "    ptv = float(centered.max() - centered.min())\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": ptv}\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def summarize_profile_numba_kernel(profile):\n",
    "        n = profile.size\n",
    "        # Compute mean\n",
    "        s = 0.0\n",
    "        for i in range(n):\n",
    "            s += profile[i]\n",
    "        mean = s / n\n",
    "        # Compute RMS and min, max of centered profile\n",
    "        s2 = 0.0\n",
    "        min_c = 1e30\n",
    "        max_c = -1e30\n",
    "        for i in range(n):\n",
    "            c = profile[i] - mean\n",
    "            s2 += c * c\n",
    "            if c < min_c:\n",
    "                min_c = c\n",
    "            if c > max_c:\n",
    "                max_c = c\n",
    "        rms = math.sqrt(s2 / n)\n",
    "        ptv = max_c - min_c\n",
    "        return mean, rms, ptv\n",
    "\n",
    "def summarize_profile_numba(profile: np.ndarray) -> dict:\n",
    "    if njit is None:\n",
    "        return summarize_profile_numpy(profile)\n",
    "    mean, rms, ptv = summarize_profile_numba_kernel(profile)\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": float(ptv)}\n",
    "\n",
    "def process_one_profile(args):\n",
    "    \"\"\"Wrapper for Pool.map: args could be (index, use_numba).\"\"\"\n",
    "    index, use_numba = args\n",
    "    profile = simulate_profile()\n",
    "    if use_numba:\n",
    "        summary = summarize_profile_numba(profile)\n",
    "    else:\n",
    "        summary = summarize_profile_numpy(profile)\n",
    "    summary[\"index\"] = index\n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    # TODO:\n",
    "    # 1) Create a list of indices, e.g. range(8)\n",
    "    # 2) Use Pool to process them in parallel\n",
    "    # 3) Print the summaries sorted by index\n",
    "    \n",
    "    # indices = ...\n",
    "    # args_list = [(i, True) for i in indices]\n",
    "    # with Pool() as pool:\n",
    "    #     results = pool.map(process_one_profile, args_list)\n",
    "    # results_sorted = sorted(results, key=lambda d: d[\"index\"])\n",
    "    # for r in results_sorted:\n",
    "    #     print(r)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97374360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution RUN FROM .py INSTEAD OF NOTEBOOK\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from multiprocessing import Pool\n",
    "\n",
    "try:\n",
    "    from numba import njit\n",
    "except ImportError:\n",
    "    njit = None\n",
    "\n",
    "def simulate_profile(n_points: int = 50_000) -> np.ndarray:\n",
    "    x = np.linspace(0.0, 10.0, n_points)\n",
    "    base = 5.0 * np.sin(2 * np.pi * x / 5.0)\n",
    "    noise = np.random.normal(loc=0.0, scale=0.5, size=n_points)\n",
    "    return base + noise\n",
    "\n",
    "def summarize_profile_numpy(profile: np.ndarray) -> dict:\n",
    "    mean = profile.mean()\n",
    "    centered = profile - mean\n",
    "    rms = math.sqrt((centered * centered).mean())\n",
    "    ptv = float(centered.max() - centered.min())\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": ptv}\n",
    "\n",
    "if njit is not None:\n",
    "    @njit\n",
    "    def summarize_profile_numba_kernel(profile):\n",
    "        n = profile.size\n",
    "        s = 0.0\n",
    "        for i in range(n):\n",
    "            s += profile[i]\n",
    "        mean = s / n\n",
    "        s2 = 0.0\n",
    "        min_c = 1e30\n",
    "        max_c = -1e30\n",
    "        for i in range(n):\n",
    "            c = profile[i] - mean\n",
    "            s2 += c * c\n",
    "            if c < min_c:\n",
    "                min_c = c\n",
    "            if c > max_c:\n",
    "                max_c = c\n",
    "        rms = math.sqrt(s2 / n)\n",
    "        ptv = max_c - min_c\n",
    "        return mean, rms, ptv\n",
    "\n",
    "def summarize_profile_numba(profile: np.ndarray) -> dict:\n",
    "    if njit is None:\n",
    "        return summarize_profile_numpy(profile)\n",
    "    mean, rms, ptv = summarize_profile_numba_kernel(profile)\n",
    "    return {\"mean\": float(mean), \"rms\": float(rms), \"ptv\": float(ptv)}\n",
    "\n",
    "def process_one_profile(args):\n",
    "    index, use_numba = args\n",
    "    profile = simulate_profile()\n",
    "    if use_numba:\n",
    "        summary = summarize_profile_numba(profile)\n",
    "    else:\n",
    "        summary = summarize_profile_numpy(profile)\n",
    "    summary[\"index\"] = index\n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    indices = list(range(8))\n",
    "    args_list = [(i, True) for i in indices]\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_one_profile, args_list)\n",
    "    results_sorted = sorted(results, key=lambda d: d[\"index\"])\n",
    "    for r in results_sorted:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e1a87",
   "metadata": {},
   "source": [
    "## Day 3 summary\n",
    "\n",
    "Today you:\n",
    "\n",
    "- Built a mental model of **CPU-bound** vs **I/O-bound** workloads.\n",
    "- Reviewed the impact of the **GIL** on threads and why processes are used for CPU-bound speedups.\n",
    "- Used **threads** for concurrent I/O-style tasks, collecting results safely with locks.\n",
    "- Used **multiprocessing** to process independent batches of measurement data in parallel.\n",
    "- Learned the basics of **AsyncIO** and coordinated multiple async measurement coroutines.\n",
    "- Revisited **NumPy**, created arrays, applied vectorized transformations, and computed statistics for physics/measurement data.\n",
    "- Practiced boolean masks, axis-wise aggregations, and small vectorized physics-style calculations.\n",
    "- Used **Numba** to JIT-compile custom numeric kernels and understood how it complements NumPy.\n",
    "- Saw an overview of **GPU tools** like [CuPy](https://cupy.dev/) and [cuDF](https://docs.rapids.ai/api/cudf/stable/) for GPU acceleration.\n",
    "- Discussed **Python 3.13** and its experimental JIT and optional no-GIL builds, and how future CPython versions may affect performance.\n",
    "- Combined multiple concepts in a complex example: parallel processing of simulated surface profiles with NumPy, Numba, and multiprocessing.\n",
    "\n",
    "These tools and concepts form a practical toolbox for high-performance numerical work in Python, especially in physics and engineering contexts. On the next days you can build on this knowledge for more advanced machine learning and deep learning workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fabb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
